[{"setId": "CoNLL-2011", "papers": [{"summaries": ["Similar to existing coreference systems such as Bengston and Roth (2008) and Rahman and Ng (2011), we perform coreference resolution using greedy left-to-right pairwise mention classification, clustering each mention with its highest-scoring antecedent (or leaving it as a singleton temporarily if no score is above a threshold).", "Our approach instead performs joint inference between within-document coreference and entity linking, maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference."], "paperId": "W13-3517", "paperName": "2011-1", "document": ["Coreference resolution systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources.", "However, since alignment itself is a challenging task and is often noisy, existing systems either align conservatively, resulting in very few links, or combine the attributes of multiple candidates, leading to a conflation of entities.", "Our approach instead performs joint inference between within-document coreference and entity linking, maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference.", "Further, we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity.", "These forms of global context enables our system to improve classifier-based coreference by 1.09 B 3 F1 points, and improve over the previous state-of-art by 0.41 points, thus introducing a new state-of-art result on the ACE 2004 data.", "Coreference resolution is the task of identifying sets of noun phrase mentions from a document that refer to the same real-world entities.", "For example, in the following excerpt: \u201cThe Chicago suburb of Arlington Heights is the first stop for hGeorge W. Bushi 1 today.", "hThe Texas governori 2 stops in hGore\u2019s home statei 3 of hTennesseei 4 this afternoon. . . \u201d,  MATH-w-2-1-0-71  define the coreferent pairs.", "Coreference resolution forms an important component for natural language processing and information extraction pipelines due to its utility in relation extraction, cross-document coreference, text summarization, and question answering.", "The task of coreference is challenging for automated systems as the local information contained in the document is often not enough to accurately disambiguate mentions, for example, coreferencing  MATH-w-2-3-0-43  requires identifying that George W. Bush ( MATH-w-2-3-0-58  ) is the governor of Texas ( MATH-w-2-3-0-67 .", "External knowledge-bases such as FrameNet ( CITE-p-24-1-1 ), Wikipedia, Yago ( CITE-p-24-3-18 ), and Freebase (Bol- lacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation.", "Incorporating external knowledge bases into coreference has been the subject of active recent research.", "Ponzetto and  CITE-p-24-3-6  and Ratinov and  CITE-p-24-3-10  precompute a fixed alignment of the mentions to the knowledge base entities.", "The attributes of these entities are used during coreference by incorporating them in the mention features.", "Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking.", "Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and  CITE-p-24-3-10 ).", "Alternatively, Rahman and  CITE-p-24-3-3  link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features.", "Although this approach is more robust to noise in the documents, the features of a mention merge the different aspects of the entities, for example a \u201cMichael Jordan\u201d mention will contain features for both the scientist and basketball personas.", "Instead of fixing the alignment of the mentions to the knowledge base, our proposed approach maintains a ranked list of candidate entities for each mention.", "To expand the set of surface strings that may be used to refer to each entity, the attributes of each candidate contain anchor texts (the visible text) of the links on the web that refer to that entity candidate.", "When mentions are compared during inference, we use the features computed from the top ranked entity candidate of the antecedent mention.", "As mentions are merged, the ranked lists of candidate entities are also merged and reranked, often changing the top-ranked entity candidate used in subsequent comparisons.", "The large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions.", "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 153\u2013162, Sofia, Bulgaria, August 8-9 2013.", "2013c Association for Computational Linguistics", "Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KB- entities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web ( CITE-p-24-3-15 ), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B 3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system ( CITE-p-24-3-16 ) by 0.41 B 3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and  CITE-p-24-3-10 , and documents that contain a large number of mentions.", "In this section we describe a variant of a commonly-used coreference resolution system that does not utilize external knowledge sources.", "This widely adopted model casts the problem as a series of binary classifications ( CITE-p-24-3-14 ,  CITE-p-24-3-4 ,  CITE-p-24-3-6 ,  CITE-p-24-1-2 ,  CITE-p-24-3-17 ).", "Given a document with its mentions, the system iteratively checks each mention  MATH-w-3-1-0-83  for coreference with preceding mentions using a classifier.", "A coreference link may be created between  MATH-w-3-1-0-102  and one of these preceding mentions using one of the following strategies.", "The C LOSEST L INK ( CITE-p-24-3-14 ) method picks the closest mention to  MATH-w-3-1-0-137  that is positively classified, while the B EST L INK ( CITE-p-24-3-4 ) method links  MATH-w-3-1-0-159  to the preceding mention that was scored the highest.", "If none of the preceding mentions are classified as positive (for C LOSEST L INK ), or are above a threshold (for B EST L INK ), then  MATH-w-3-5-0-40  is left unlinked.", "After all the mentions have been processed, the links are used to generate a transitive closure that corresponds to the recognized entities in the document.", "The features used to train our classifier are similar to those in Bengston and  CITE-p-24-1-2 , including lexical, syntactical, semantic, predicted NER types, etc., with the exclusion of their \u201clearned features\u201d that require additional classifiers.", "Further, we include features that compare the mention strings, the distance between the two mentions in terms of the number of sentences and tokens, and the POS tags of the head words.", "We also use the conjunctions of these features as in Bengston and  CITE-p-24-1-2 , as well as the B EST L INK approach.", "The training for our system is similar to Bengston and  CITE-p-24-1-2 .", "The positive training examples are generated from mentions and their immediate preceding antecedent.", "The negative examples are generated from mentions and all their preceding non-coreferent mentions.", "If the mention is not a pronoun, preceding pronouns are not used to create training examples, and they are also excluded during inference.", "In contrast to averaged perceptron used in Bengston and  CITE-p-24-1-2 , our baseline system is trained using hinge-loss,  MATH-w-4-1-1-94  - regularized SVM.", "When a mention  MATH-w-5-1-0-3  is compared against a preceding mention  MATH-w-5-1-0-12  , information from other mentions that are already coreferent with  MATH-w-5-1-1-5  may be helpful in disambiguating  MATH-w-5-1-1-12  as they may contain information that is not available from  MATH-w-5-1-1-25  .", "Let  MATH-w-5-1-1-29  be the mentions between  MATH-w-5-1-1-34  and  MATH-w-5-1-1-37  that are coreferent with  MATH-w-5-1-1-44  .", "Let  MATH-w-5-1-1-48  be the mention that is closest to  MATH-w-5-1-1-59  .", "All the features from the pair  MATH-w-5-1-1-68 , except those that characterize one mention (for example, mention type of  MATH-w-5-1-1-90  ), are added to the features between  MATH-w-5-1-1-100 .", "This extends a similar approach by  CITE-p-24-3-3  that merges only the attributes of mentions (such as gender, but not all pairwise features).", "A potential drawback of including all the negative examples as in Bengston and  CITE-p-24-1-2  is that the negative instances far outnumber the positive ones, which is challenging for training a classifier.", "In their system, the positive training examples only constitute 1.6% of the total training instances.", "By contrast,  CITE-p-24-3-14  reduce the number of negative instances by using only mentions between the mention and its closest coreferent pair as negative examples.", "Instead of just using the closest coreferent mention, we extend this approach to use the  MATH-w-6-1-0-105  closest of coreferent preceding mentions, where  MATH-w-6-1-0-113  is tuned using the development data.", "In this section, we describe our approach to coreference resolution that incorporates external knowledge sources.", "The approach is an extension of the pairwise model described earlier, with the inclusion of a ranked list of entities, and using a larger set of surface string variations.", "We describe our overall approach in Algorithm 1.", "The system assumes that the data is annotated with true mention boundaries and mention types.", "We additionally tokenize the document text and tag the tokens with their parts of speech for use as features.", "First, an empty entity candidate list is created for each mention in the document.", "For each proper noun mention, we query a knowledge base for an ordered list of Wikipedia articles that may refer to it, and add these to the mention\u2019s candidate list.", "After this preprocessing, each mention  MATH-w-8-1-1-6  is compared against its preceding mentions  MATH-w-8-1-1-14  and their top-ranked entity candidate using a classifier.", "Amongst antecedents  MATH-w-8-4-0-7  that score above a threshold, the highest-scoring one  MATH-w-8-4-0-23  is marked as coreferent with  MATH-w-8-4-0-30  and the two candidate lists that correspond to  MATH-w-8-4-0-40  and  MATH-w-8-4-0-43  are merged.", "Merging two mentions results in the merging and reranking of their respective entity candidate lists, described below.", "If no antecedents score above a threshold, we leave the mention in its singleton cluster.", "Algorithm 1 Dynamic Linking to Wikipedia 1: Input: Mentions {m j } 2: Initialize blank entity lists {E m } .", "Section 3.2 3: for m \u2208 Proper Noun Mentions do 4: L INK W IKIPEDIA (m, E m ) .", "Section 3.2 5: P OPULATE E NTITY A TTRS (E m ) .", "Section 3.3 6: end for 7: for m i \u2208 Mentions do 8: Antecedents \u2190 {m 1 ...m i\u22121 } 9: for m\u0302 \u2208 Antecedents do 10: t \u2190 T OP R ANKED A TTRS (E m\u0302 ) .", "Section 3.4 11: s \u2190 S CORE (m\u0302, m i , t) .", "Section 3.4 12: Scores m\u0302 \u2190 s 13: end for m \u2217 \u2190 arg max m\u0302 Scores m\u0302 14: 15: if Scores m \u2217 > threshold then 16: M ARK C OREFERENT (m \u2217 , m i ) 17: M ERGE E NTITY L ISTS (E m \u2217 , E m i ) .", "To create the initial entity candidate lists for proper noun mentions, we query a knowledge base searcher ( CITE-p-24-1-6 ) with the text of these mentions.", "These queries return scored, ranked lists of entity candidates (Wikipedia articles), which we associate with each proper noun mention, leaving the rest of the candidate lists empty.", "Linking is often noisy, so only selecting the high-precision links as in Ratinov and  CITE-p-24-3-10  results in too few matches, while picking an aggregation of all links results in more noise due to lower precision ( CITE-p-24-3-9 ).", "Additionally, since linking is often performed in preprocessing, two mentions that are determined coreferent during inference could still be linked to different KB entities.", "To avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference.", "After linking to Wikipedia, we have a list of candidate KB entities for each mention.", "Each entity has access to external information keyed on the Wikipedia article, but this information could more generally come from any knowledge base.", "Given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine-grained Wikipedia categories as used by Ratinov and  CITE-p-24-3-10 , however most of these features may not be relevant to the task of within-document coreference.", "Instead, an important resource for linking non-proper mentions of an entity is to identify the possible name variations of the entity.", "For example, it would be useful to know that Massachusetts is also referred to as \u201cThe 6th State\u201d, however this information is not readily available from Wikipedia.", "We instead use the corpus described in Spitkovsky and  CITE-p-24-3-15  that consists of anchor texts of links to Wikipedia that appear on web pages.", "This collection of anchor texts is sufficiently extensive to cover many common misspellings of entity names, as well as many name variations missing from Wikipedia.", "For example, for the entity \u201cMassachusetts\u201d, our anchor texts include misspellings like \u201cMassachussetts\u201d and \u201cMessuchusetts\u201d, and the (debatably) affectionate nickname of \u201cTaxachusetts\u201d\u2014none of which are found in Wikipedia.", "Using these anchor texts, each entity candidate provides a rich set of name variations that we use for disambiguation, as described in the next section.", "The input to our inference algorithm consists of a number of mentions, a list of ranked entity candidates for the proper noun mentions that are present in the KB, and a list of attributes (in this case, name variations) for each entity candidate.", "Scoring: Our underlying model is a pairwise classification approach as described in Section 2.", "Similar to existing coreference systems such as Bengston and  CITE-p-24-1-2  and Rahman and  CITE-p-24-3-3 , we perform coreference resolution using greedy left-to-right pairwise mention classification, clustering each mention with its highest-scoring antecedent (or leaving it as a singleton temporarily if no score is above a threshold).", "We add the same additional features and perform feature merging operation (Section 2.2) as in our baseline system.", "The top-ranked entity candidate of the antecedent mention is used during coreference to provide additional features for the pairwise classifier.", "Only using the top-ranked entity candidate allows the system to maintain a consistent one entity per cluster hypothesis, reducing the noise resulting from conflated entities.", "The attributes for this top-ranked entity consist of name variations.", "We add a binary feature, and conjunctions of this with other features, if the text of the right mention matches one of these name variations.", "Entity List Merging: Once a mention pair is scored as coreferent, their corresponding entity candidates are merged.", "Merging is performed by simply combining the two lists of candidates.", "Note that there is only one candidate list for a given group of coreferent mentions at any point in inference: if  MATH-w-11-4-1-56  and  MATH-w-11-4-1-59  have been previously marked as coreferent, and  MATH-w-11-4-1-69  is marked as coreferent with  MATH-w-11-4-1-76  ,  MATH-w-11-4-1-79  \u2019s entity candidates will then contain those from m 3 for future classification decisions.", "Re-Ranking: After the two entity candidate lists are merged, we rerank the candidates to identify the top-ranked one.", "We sort the new list of candidate entities by the number of times each candidate occurs in the list, breaking ties by their original relevance from the KB.", "For example, if two mentions disagree on the top-ranked KB search result, but agree on the second one, after being clustered they will both use the second search result when creating feature vectors for future coreference decisions.", "Even though other candidates besides the top-ranked one are ignored for a single classification decision, they may become top-ranked after merging with later candidate sets.", "This approach allows our system to use the intermediate results of coreference resolution to re-link mentions to KB entities, reducing the noise and contradictory features from incorrect links.", "Additionally, features from the KB are added to non-proper noun mentions once those mentions are linked with a populated entity, allowing the results of coreference to enrich non-proper noun mentions with KB-based features.", "The initial proper noun queries effectively seed the linking process, and KB data is then dynamically spread to the other mentions through coreference.", "We describe a run of our approach on an example in Figure 1.", "Consider three mentions, each paired with a top-ranked KB candidate: \u201cWashington\u201d, \u201cWash\u201d, and \u201cWashington State\u201d.", "For the first two mentions, clearly the top entity candidate is incorrect; hence approaches that rely on a fixed alignment will perform poorly.", "In particular, since \u201cWashington State\u201d mention is not compatible with the top-ranked entities of the first two mentions (Washington, D.C. and Car Wash respectively), approaches that do not modify the ranking during inference may not resolve them.", "However, the correct candidate Washington State does appear in the candidate entities of the first two mentions, albeit with a lower rank.", "In our approach, clustering the first two mentions causes the shared candidate Washington State to move to the top of the list.", "The coreference system is now able to easily identify that the \u201cWashington State\u201d mention is compatible with the Washington State entity formed by the previous two mentions, providing evidence that the final mention should be clustered with either of them in subsequent comparisons.", "\u2026about navigation charts that he had ordered from a company based in the Washington state of Washington.", "He assumed \u2026 \u2026opened one of them to discover the absentee ballot of Steven H. Forrester Wash of Bellevue, Wash\u2026.", "...were not meaningful because Washington counting in Washington State has been completed... State (a) Example Excerpts with Mentions Washington, DC Washington State Washington Washington State ... Washington, DC Car Wash Car Wash The Wash The Wash ... ... Wash Washington State Washington State Washington Washington State ... ... State (b) Initial Alignment (top-ranked in bold) (c) Merged and Reranked Alignment", "We evaluate our system on the ACE 2004 annotated dataset ( CITE-p-24-1-8 ).", "Following the setup in Bengston and  CITE-p-24-1-2 , we split the corpus into training, development, and test sets, resulting in 268 documents in the train set, 107 documents in the test set, and 68 documents in the development set.", "The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP 2 tagger to obtain the POS tags.", "The hyperparameters of our system, such as regularization, initial number of candidates, and the number of comparisons during training ( MATH-w-14-4-0-4  in Section 2.3) are tuned on the development data when trained on the train set.", "The models we use to evaluate on the test data set are trained on the training and development sets, following the standard evaluation for coreference first used by  CITE-p-24-1-5 .", "To provide the initial ranked list of entity candidates from Wikipedia, we query the KB Bridge system ( CITE-p-24-1-6 ) with the proper name mentions.", "KB Bridge is an information-retrieval- based entity linking system that connects the query mentions to Wikipedia entities using a sequential dependence model.", "This system has been shown to match or outperform the top performing systems in the 2012 TAC KBP entity linking task.", "Our experiments investigate a number of baselines that are similar or identical to existing approaches.", "Wikipedia Linking: As a simple baseline, we directly evaluate the quality of the alignment for coreference by merging all pairs of proper noun mentions that share at least one common candidate, as per KB bridge.", "Further, the non-pronoun mentions are linked to these proper nouns if the mention string matches any of the entity titles or anchor texts.", "Bengston and  CITE-p-24-1-2 : A pairwise coreference model containing a rich set of features, as described and evaluated in Bengston and  CITE-p-24-1-2 .", "Baseline: Our implementation of a pairwise model that is similar to the approach in Bengston and  CITE-p-24-1-2  with the differences described in Section 2.", "This is our baseline system that performs coreference without the use of external knowledge.", "Dynamic linking: This is our complete system as described in Section 3, in which the list of candidates associated with each mention is reranked and modified during inference.", "Static linking: Identical to dynamic linking except that entity candidate lists are not merged during inference (i.e., Algorithm 1 without line 17).", "This approach is comparable to the fixed alignment model, as in the approaches of Ponzetto and  CITE-p-24-3-6  and Ratinov and  CITE-p-24-3-10 .", "As in Bengston and  CITE-p-24-1-2 , we evaluate our system primarily using the B 3 metric ( CITE-p-24-1-0 ), but also include pairwise, MUC and CEAF(m) metrics.", "The performance of our systems on the test data set is shown in Table 2.", "These results use true mentions provided in the dataset, since, as suggested by  CITE-p-24-3-5 , coreference resolvers that use different mention detectors (extraction from parse tree, detector trained from gold boundaries, etc.) should not be compared.", "Our baseline system outperforms Bengston and  CITE-p-24-1-2  by 0.32 B 3 F1 points on this data set.", "Incorporating Wikipedia and anchor text information from the web with a fixed alignment (static linking) further improves our performance by 0.54 B 3 F1 points.", "Using dynamic linking, which improves the alignment during inference, achieves another 0.55 F1 point improvement, which is 1.09 F1 above our baseline, 1.41 F1 above the current best pairwise classification system (corresponding to an error reduction of  MATH-w-16-1-1-103 ), and 0.4 F1 above the current state-of-art on this dataset ( CITE-p-24-3-16 ).", "The improvement of the dynamic linking approach over our baselines is consistent across the various evaluation metrics.", "We also explore our system\u2019s performance on subsets of the ACE dataset, and on the OntoNotes dataset.", "Coreference becomes more difficult as the number of mentions is increased since the number of pairwise comparisons increases quadratically with the number of mentions.", "We observe this phenomenon in our dataset: the performance on the smallest third of the documents (when sorted according to number of mentions) is 8.5-10% higher than on the largest third of the documents, as per the B 3 metric.", "However, we expect dynamic linking of entities to be more beneficial on these larger documents as our system can use the information from a larger number of mentions to improve the alignment during inference.", "Static linking, on the other hand, is unlikely to obtain higher improvements with the larger number of mentions in the document as the alignment is fixed.", "We perform the following experiment to analyze the performance with varying numbers of mentions.", "We sort all the documents in the test set according to their number of mentions, and evaluate on the top  MATH-w-18-7-1-36  of this list (where  MATH-w-18-7-1-43 ).", "As the results demonstrate in Figure 2, the improvement of the static linking approach stays fairly even as  MATH-w-18-7-1-73  is varied.", "Even though the experiments suggest that the larger documents are tougher to coreference, 3 dynamic linking provides higher improvements when the documents contain a larger number of mentions.", "The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document.", "In particular, ACE contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies.", "Since these transcripts provide an additional challenge for alignment and coreference, Ratinov and  CITE-p-24-3-10  only use the set of non-transcripts for their evaluation.", "Using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts.", "To identify the transcripts in the test set, we use the approximation from Ratinov and  CITE-p-24-3-10  that considers a document to be non-transcribed if it contains proper noun mentions and at least a third of those start with a capital letter.", "The performance is shown in Table 3, while the improvement over our baseline is shown in Figure 3.", "Our static linking matches the performance of Ratinov and  CITE-p-24-3-10  on the non-transcripts.", "Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment.", "Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non-transcripts.", "This indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts.", "We also run our systems on the OntoNotes dataset, which was used for evaluation in CoNLL 2011 Shared Task ( CITE-p-24-3-7 ).", "The dataset consists of 2083 documents from a much larger variety of genres, such as conversations, magazines, web text, etc.", "Further, the dataset also consists of mentions that refer to events, most of which do not appear as Wikipedia pages.", "Since only the non-singleton mentions are annotated in the training set, we also include additional noun phrase mentions during training.", "We obtain B 3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.", "4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on this metric, suggesting that dynamic alignment is beneficial even when the features have not been engineered for events or for different genres.", "Within-document coreference has been well-studied for a number of years.", "A variety of approaches incorporate linguistic knowledge as rules iteratively applied to identify the chains, such as Haghighi and  CITE-p-24-1-10 ,  CITE-p-24-3-8 ,  CITE-p-24-3-17 .", "Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model.", "Early applications of such models include  CITE-p-24-3-14 , Ng and  CITE-p-24-3-4  and ( CITE-p-24-1-2 ).", "There are also a number of techniques that represent entities explicitly (Culotta et al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012).", "This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference.", "Ponzetto and  CITE-p-24-3-6  identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model.", "Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one.", "Rahman and  CITE-p-24-3-3  introduce similar features from a more extensive set of knowledge sources (such as YAGO and FrameNet) into a cluster-based model whose features change as inference proceeds.", "However, the features for each cluster come from a combination of all entities aligned to the cluster mentions.", "We improve upon this approach by maintaining a list of the candidate entities for each mention cluster, modifying this list during the course of inference, and using features from only the top-ranked candidate at any time.", "Ratinov and  CITE-p-24-3-10  extend the multi-sieve coreference model ( CITE-p-24-3-8 ) by identifying at most a single candidate for each mention, and incorporating high-precision attributes extracted from Wikipedia.", "The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied.", "With these restrictions, they show improvements over the state-of- the-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts.", "There are a number of approaches that provide an alignment from mentions in a document to Wikipedia.", "Wikifier ( CITE-p-24-3-11 ) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in Ratinov and  CITE-p-24-3-10 .", "Dalton and  CITE-p-24-1-6  introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012).", "Alignment to an external knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition ( CITE-p-24-1-4 ,  CITE-p-24-3-1 ), cross-document coreference ( CITE-p-24-1-9 ,  CITE-p-24-3-13 ), and relation-extraction ( CITE-p-24-3-12 ,  CITE-p-24-3-2 ).", "In this paper, we incorporate external knowledge to improve within-document coreference.", "Instead of fixing the alignment a priori, our approach maintains a ranked list of candidate entities for each mention, and merges and reranks the list during inference.", "Further, we consider a large set of surface string variations for each entity by using anchor texts from the web.", "These external sources allow our system to achieve a new state-of-the-art on the ACE data.", "We also demonstrate improvements on documents that are difficult for alignment and coreference, such as transcripts and documents containing a large number of mentions.", "A number of possible avenues for future study are apparent.", "First, our alignment to a knowledge-base can benefit from more document-aware linking to entities, such as the Wikifier ( CITE-p-24-3-11 ).", "Second, we would like to augment mention features with additional information available from the knowledge base, such as Wikipedia categorization and gender attributes.", "We also want to investigate a cluster ranking model, as used in ( CITE-p-24-3-9 ,  CITE-p-24-3-16 ), to aggregate the features of all the coreferent mentions as inference progresses."]}, {"summaries": ["The approach we present consists of a multi-classifier system which classifies mention-pairs in a reduced dimensional vector space.", "In this paper a different machine learning approach to deal with the coreference resolution task is presented: a multi-classifier system that classifies mention-pairs in a reduced dimensional vector space created by applying the SVD technique."], "paperId": "W15-1503", "paperName": "2011-2", "document": ["In this paper a different machine learning approach is presented to deal with the coreference resolution task.", "This approach consists of a multi-classifier system that classifies mention-pairs in a reduced dimensional vector space.", "The vector representation for mention-pairs is generated using a rich set of linguistic features.", "The SVD technique is used to generate the reduced dimensional vector space.", "The approach is applied to the OntoNotes v4.0 Release Corpus for the column-format files used in CONLL-2011 coreference resolution shared task.", "The results obtained show that the reduced dimensional representation obtained by SVD is very adequate to appropriately classify mention-pair vectors.", "Coreference resolution deals with the problem of finding all expressions that refer to the same entity in a text ( CITE-p-16-1-21 ).", "It is an important subtask in Natural Language Processing that require natural language understanding, and hence, it is considered to be difficult.", "A coreference resolution system has to automatically identify the mentions of entities in text and link the corefering mentions (the ones that refer to the same entity) to form coreference chains.", "Systems are expected to perform both, mention detection and coreference resolution.", "Preliminary researches proposed heuristic approaches to the task, but thanks to the annotated coreference corpora made available in the last years and the progress achieved in statistical NLP methods, machine learning approaches to the coreference resolution task are being proposed.", "( CITE-p-16-1-24 ) presents an interesting survey of the progress in coreference resolution.", "In this paper we present a different machine learning approach to deal with the coreference resolution task.", "Given a corpus with annotated mentions, the multi-classifier system we present classifies mention-pairs in a reduced dimensional vector space.", "We use the typical mention-pair model, where each pair of mentions is represented by a rich set of linguistic features; positive instances correspond to mention-pairs that corefer.", "Coreference resolution is tackled as a binary classification problem ( CITE-p-16-3-7 ) in this paper; the subsequent linking of mentions into coreference chains is not considered.", "In fact, the aim of our experiment is to measure to what extent working with feature vectors in a reduced dimensional vector space and applying a multi-classifier system helps to determine the coreference of mention-pairs.", "To the best of our knowledge, there are no approaches to the coreference resolution task which make use of multi-classifier systems to classify mention-pairs in a reduced dimensional vector space.", "This paper gives a brief description of our approach to deal with the problem of identifying whether two mentions corefer and shows the results obtained.", "Section 2 presents related work.", "In Section 3 our approach is presented.", "Section 4 presents the case study, where details about the dataset used in the experiments and the preprocessing applied are given.", "In Section 5 the experimental setup is briefly introduced.", "The experimental results are presented and discussed in Section 6, and finally, Section 7 contains some conclusions and comments on future work.", "Proceedings of NAACL-HLT 2015, pages 17\u201324, Denver, Colorado, May 31 \u2013 June 5, 2015.", "2015c Association for Computational Linguistics", "Much attention has been paid to the problem of coreference resolution in the past two decades.", "Conferences specifically focusing coreference resolution have been organized since 1995.", "The sixth and seventh Message Understanding Conferences (MUC- 6, 1995; MUC-7, 1998) included a specific task on coreference resolution.", "The Automatic Context Extraction (ACE) Program focused on identifying certain types of relations between a predefined set of entities ( CITE-p-16-1-12 ) while the Anaphora Resolution Exercise (ARE) involved anaphora resolution and NP coreference resolution ( CITE-p-16-1-26 ).", "More recently, SemEval-2010 Task 1 was dedicated to coreference resolution in multiple languages.", "One year later, in the CoNLL-2011 shared task ( CITE-p-16-3-1 ), participants had to model unrestricted coreference in the English-language OntoNotes corpora and CoNLL-2012 Shared Task ( CITE-p-16-3-2 ) involved predicting coreference in three languages: English, Chinese and Arabic.", "Recent work on coreference resolution has been largely dominated by machine learning approaches.", "In the SemEval-2010 task on Coreference Resolution in Multiple Languages ( CITE-p-16-3-4 ), most of the systems were based on these techniques ( CITE-p-16-1-6 ,  CITE-p-16-3-8 ,  CITE-p-16-1-16 ).", "The same occurs at CoNLL-2011, where ( CITE-p-16-1-8 ,  CITE-p-16-3-5 ) were based on machine learning techniques.", "The advantage of these approaches is that there are many open-source platforms for machine learning and machine learning based coreference systems such as BART ( CITE-p-16-3-10 ), the Illinois Coreference Package ( CITE-p-16-1-1 ) or the Stanford CoreNLP ( CITE-p-16-1-19 ), among others.", "Nevertheless, rule-based systems have also been applied successfully (Lappin et al., 1994; Mitkov, 1998; Lee et al., 2013).", "The authors of this last system propose a coreference resolution system that is an incremental extension of the multi-pass sieve system proposed by ( CITE-p-16-3-3 ).", "This system is shifting from the supervised learning setting to an unsupervised setting, and obtained the best result in the CoNLL-2011 Shared Task.", "Some very interesting uses of vector space models for the coreference resolution task can be found in the literature.", "( CITE-p-16-1-25 ) investigate the effect of using vector space models as an approximation of the kind of lexico-semantic and commonsense knowledge needed for coreference resolution for Swedish texts.", "They also work with reduced dimensional vector spaces and obtain encouraging results.", "In an attempt to increase the performance of a coreference resolution engine, ( CITE-p-16-1-7 ) make use of structured semantic knowledge available in the web.", "One of the strategies they adopt is to apply the SVD to Wikipedia articles and classify mentions in a reduced dimensional vector space.", "The approach we present consists of a multi-classifier system which classifies mention-pairs in a reduced dimensional vector space.", "This multi-classifier is composed of several kNN classifiers.", "A set of linguistic features is used to generate the vector representations for the mention-pairs.", "The training dataset is used to create a reduced dimensional vector space using the SVD technique.", "Mention- pairs in the training, development and test sets are represented using the same linguistic features and projected onto the reduced dimensional space.", "The classification process is performed in the reduced dimensional space.", "To create the multi-classifier, we apply random subsampling and obtain training datasets  MATH-w-4-1-1-25  for the reduced dimensional space.", "Given a testing case  MATH-w-4-1-1-47 , the kNN classifier makes a label prediction  MATH-w-4-1-1-56  based on each one of the training datasets  MATH-w-4-1-1-66  , and predictions  MATH-w-4-1-1-72  are combined to obtain the final prediction  MATH-w-4-1-1-87  using a Bayesian voting scheme.", "It is a binary classification system where the final prediction  MATH-w-4-1-1-105  may be positive (mentions tested corefer) or negative (mentions do not corefer).", "Figure 1 shows an illustration of the fundamental steps of the experiment.", "In the rest of this section, details about the SVD dimensionality reduction technique, the kNN classification algorithm, the combination of classifiers and the evaluation measures used are briefly reviewed.", "The classical Vector Space Model (VSM) has been successfully employed to represent documents in text categorization and Information Retrieval tasks.", "Latent Semantic Indexing (LSI) 1 ( CITE-p-16-1-10 ) is a variant of the VSM in which documents are represented in a lower dimensional vector space created from a training dataset.", "To create such a lower dimensional vector space, LSI generates a term-document matrix  MATH-w-5-1-0-78  and computes its SVD matrix decomposition,  MATH-w-5-1-0-86  .", "As a result,  MATH-w-5-1-0-97  singular values are obtained, and terms and documents are mapped to the r-dimensional vector space.", "By reducing the  MATH-w-5-1-0-118  to  MATH-w-5-1-0-120 , a reduced dimensional space is created, the p-dimensional space onto which vectors are projected.", "This reduced dimensional space is used for classification purposes, and the cosine similarity is usually used to measure the similarity between vectors ( CITE-p-16-1-2 ).", "It has been proved that computing the similarity of vectors in the reduced dimensional space gives better results than working in the original space.", "In fact, LSI is said to be able to capture the latent relationships among words in documents thanks to the word co-occurrence analysis performed by the SVD technique, and therefore, cluster semantically terms and documents.", "This powerful technique is being used to better capture the semantics of texts in applications such as Information Retrieval ( CITE-p-16-1-3 ).", "LSI is referred to as Latent Semantic Analysis (LSA) when it is used as a model of the acquisition, induction and representation of language and the focus is on the analysis of texts ( CITE-p-16-1-13 ).", "For the sake of the coreference resolution task, each document corresponds to a mention-pair, and words in each document are the linguistic feature values for the associated mention-pair.", "Section 4.2 gives details about the linguistic features used to represent each mention-pair.", "Matrix  MATH-w-5-4-1-48  is constructed for the selected feature values (terms) and all mention-pairs considered (documents).", "The SVD decomposition is computed and the p-dimensional reduced space is created.", "We use  MATH-w-5-4-1-83  as the reduced dimensional representation, and compute the coordinates to project mention-pair vectors onto the reduced space and compare them.", "kNN is a distance based classification approach.", "According to this approach, given an arbitrary testing case, the kNN classifier ranks its nearest neighbors among the training cases, and uses the class of the  MATH-w-6-1-0-39  top-ranking neighbors to do the prediction for the testing case being analyzed ( CITE-p-16-1-9 ).", "In our experiments, parameter  MATH-w-6-1-1-5  is set to 3.", "Given a testing mention-pair vector, the 3-NN classifier is used to find the three nearest neighbor mention-pair vectors in the reduced dimensional vector space.", "The cosine is used to measure vector similarity and find the nearests.", "We also consider the kNN classifier provided with the Weka package ( CITE-p-16-1-14 ,  CITE-p-16-1-0 ).", "We use it to obtain a honest comparison for the results.", "The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components ( CITE-p-16-1-15 ).", "A widely used technique to implement this approach is bagging ( CITE-p-16-1-5 ), where a set of training datasets  MATH-w-7-1-0-53  is generated by selecting  MATH-w-7-1-0-60  training cases drawn randomly with replacement from the original training dataset  MATH-w-7-1-0-73  of  MATH-w-7-1-0-76  cases.", "When a set of  MATH-w-7-1-0-83  training cases is chosen from the original training collection, the bagging is said to be applied by random subsampling.", "In fact, this is the approach used in our work and the  MATH-w-7-1-0-121  parameter is set to be 60% of the total number of training cases  MATH-w-7-1-0-137 .", "The proportion of positive and negative cases in the training dataset  MATH-w-7-1-0-150  is preserved in the different  MATH-w-7-1-0-157  datasets generated.", "According to the random subsampling, given a testing case  MATH-w-7-1-1-10 , the classifier makes a label prediction  MATH-w-7-1-1-19  based on each one of the training datasets  MATH-w-7-1-1-29  .", "Label predictions  MATH-w-7-1-1-35  may be either positive or negative.", "One way to combine the predictions is by Bayesian voting ( CITE-p-16-1-11 ), where a confidence value cv ic j is calculated for each training dataset  MATH-w-7-1-1-73  and label to be predicted.", "These confidence values are calculated based on the training collection.", "Confidence values are summed by label; the label  MATH-w-7-1-1-103  that gets the highest value is finally proposed as a prediction for the testing case  MATH-w-7-1-1-121 .", "The approach presented in this paper is a binary classification system where the final prediction  MATH-w-8-1-0-15  may be positive (mentions tested corefer) or negative (mentions do not corefer).", "There are many metrics that can be used to measure the performance of a classifier.", "In binary classification problems precision and recall are very widely used.", "Precision (Prec) is the number of correct positive results divided by the number of all positive results, and recall (Rec) is the number of correct positive results divided by the number of positive results that should have been returned.", "In general, there is a trade-off between precision and recall.", "Thus, a classifier is usually evaluated by means of a measure which combines them.", "The  MATH-w-8-1-1-29  - score can be interpreted as a weighted average of precision and recall; it reaches its best value at 1 and worst score at 0.", "Accuracy is also used as a statistical measure of performance in binary classification tasks.", "Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases tested.", "This section briefly reviews the dataset used in the experiments and the preprocessing applied.", "The OntoNotes v4.0 Release Corpus is used in the experiments 2 .", "It provides a large-scale multi-genre corpus with multiple layers of annotation (syntactic, semantic and discourse information) which also include coreference tags.", "A nice description of the coreference annotation in OntoNotes can be found in ( CITE-p-16-1-27 ) and ( CITE-p-16-3-0 ).", "Although OntoNotes is a multilingual resource for English, Chinese and Arabic, for the scope of this paper, we just look at the English portion.", "We use English texts for five different genres or types of sources: broadcast conversations (BC), broadcast news (BN), magazine articles (MZ), newswires (NW) and web data (WB).", "The English language portion of the OntoNotes v4.0 Release Corpus was used in the CONLL-2011 coreference resolution Shared task 3 .", "The task was to automatically identify mentions of entities and events in text and to link the corefering mentions together to form mention chains ( CITE-p-16-3-1 ,  CITE-p-16-3-2 ).", "Since OntoNotes coreference data spans multiple genre, the task organizers created a test set spanning all the genres.", "The training, development and test files were downloaded from the CONLL-2011 website, and the * conll files were generated from each corresponding * skel files using the scripts made available by the organizers.", "The * conll files contain information in a tabular structure where the last column contains coreference chain information.", "Two types of * conll files may be generated, depending on how the annotation was generated; *gold conll files were hand-annotated and adjudicated quality, whereas annotations in *auto conll files were produced using a combination of automatic tools.", "*gold conll files are used in the experiments presented in this paper.", "In order to obtain the vector representation for each pair of mentions, we used the features defined by ( CITE-p-16-3-6 ).", "The 127 binary features they define are related to distance, position, lexical information, morphological information, syntactic dependencies and semantic features.", "The authors developed a coreference resolution system called Re- laxCor 4 and participated in the CoNLL-2011 shared task obtaining very good results.", "RelaxCor is a constraint-based hypergraph partitioning approach to coreference resolution, solved by relaxation labeling.", "It generates feature vectors for all mention-pairs in the * conll files as part of the system and uses them to solve the task.", "We decided to use the perl scripts distributed by the authors and generate the positive and negative feature vectors for all * conll files.", "These feature vectors consist of binary values for the 127 binary features and a label: a positive label (+) indicates that the feature vector corresponds to a corefering mention-pair, whereas a negative label (-) indicates that the two mentions do not corefer.", "Note that each mention in a file is combined with all the rest of mentions in the same file to form mention-pairs and consequently, a very large amount of negative examples is generated, specially for large files.", "We decided to reduce the amount of negative examples, in a similar manner as ( CITE-p-16-3-6 ) and therefore, negative examples with more than five feature values different from any positive example in each file were eliminated.", "In order to obtain the training, development and test corpora for the 5 genres, we brought together the examples generated from files of the same split and genre.", "We removed contradictions (negative examples with identical feature values as a positive example) and examples that appeared more than once in the same corpus.", "We noticed that the size of the corpora was too large for some of the genres; the broadcast conversations (BC) genre training corpus for instance had more than 4 million examples.", "Table 1.", "gives detailed information about the number of positive and negative mention-pairs in the training, development and test corpora used in the experiments.", "A matrix is constructed for each of the training corpus.", "Feature values that appear at least once in the corpus are selected as terms.", "Even though theoretically we could have a maximum number of 254 different terms in each training corpus (127 \u00d7 2, because the 127 features are binary), the real value is between 227 and 230.", "The sizes of the matrices created are given by the number of terms and documents (sum of (+) and (-) examples in the training corpus) and can be seen in Table 2.", "To optimize the behaviour of the multi-classifier system, the number of  MATH-w-12-1-0-13  training datasets is adjusted in a parameter tuning phase.", "This optimization process is performed in an independent way for each of the genres because the five genres correspond to texts coming from different sources and may have very different characteristics ( CITE-p-16-3-9 ).", "Therefore, we treat them as five different classification problems.", "The five development corpora are used to adjust parameter  MATH-w-12-1-1-9  (the amount of  MATH-w-12-1-1-14  training datasets).", "We experimented with the following values for  MATH-w-12-1-1-28 : 5, 10, 20, 30, 40, 50, 60, 70, 80.", "Table 3 shows the optimal values obtained for each genre.", "This means that testing cases for the BC genre, for instance, are classified by a multi-classifier formed by 60 kNN classifiers, after having generated 60  MATH-w-12-1-1-87  training datasets from the original  MATH-w-12-1-1-95 .", "Two different dimensional representations are experimented for mention-pair vectors.", "On the one hand, we consider mention-pair vectors represented in the original 127 dimensions.", "On the other hand, the SVD-computed dimensional vector representation is being experimented.", "Table 3 shows the number of singular values (dimensions) computed by SVD for each of the genres.", "Three experiments were carried out in the test phase using the optimal values for parameter  MATH-w-13-1-0-15  and the two different representations for mention-pair vectors.", "Table 4 shows the results obtained for each of the experiments: accuracy values in a first row (Acc.)", "and  MATH-w-13-1-0-49  -scores in a second ( MATH-w-13-1-0-56  ).", "In a first experiment (Exp.1), the Weka 3-NN classifier is applied to classify testing cases represented in the original 127 dimensional space.", "The same 3-NN classifier is applied in a second experiment (Exp.2), but training and testing cases are represented using the dimensions computed by SVD (see Singular Values in Table 3).", "In a last experiment (Exp.3), our approach is applied and a multi-classifier system classifies testing vectors in the same SVD-dimensional vector space as in the previous experiment.", "The multi-classifier is generated according to the optimal values for parameter  MATH-w-13-1-1-115  in each genre.", "The results shown in bold in the first part of Table 4 are the best for each genre.", "Note that the two performance measures computed (accuracy and  MATH-w-13-4-0-30  - score) are very correlated in the five cases.", "Taking into account that the proportion of positive and negative examples varies from genre to genre, this correlation gives consistency to the interpretation of the results obtained.", "The best results for BC and MZ genres are obtained in the first experiment, applying the 3-NN classifier to the 127 dimensional vectors (Exp.1,  MATH-w-13-4-1-30  - scores: 0.762 and 0.731, respectively).", "For the rest of the genres, the best results are obtained for the SVD-dimensional vectors.", "An  MATH-w-13-4-1-64  -score of 0.85 is obtained for the WB genre in the second experiment (Exp.2).", "The approach proposed in this paper (Exp.3) achieves the best results for two out of the five genre, with an  MATH-w-13-4-2-41  -score of 0.728 for BN and 0.716 for NW.", "The last column in Table 4 shows the mean accuracy and  MATH-w-13-4-3-12  -scores obtained in each experiment, taking into account the five genres as a whole (the best are shown in bold).", "The best mean  MATH-w-13-4-3-41  -score is obtained in Experiment 2, where vectors are classified in the SVD-dimensional vector space.", "In fact, this result is very closely followed by the one obtained in Experiment 3 with our approach, (mean  MATH-w-13-4-3-84  -scores: 0.747 and 0.746, respectively).", "The best mean accuracy is obtained when our approach is applied (mean accuracy: 0.721).", "This good results seem to suggest that the dimensions computed by the SVD technique are very appropriate to represent mention-pairs and classify them.", "Moreover, the use of the multi-classifier system gets to achieve even better results, outperforming the ones obtained by the other classification systems.", "In this paper a different machine learning approach to deal with the coreference resolution task is presented: a multi-classifier system that classifies mention-pairs in a reduced dimensional vector space created by applying the SVD technique.", "The results obtained for the OntoNotes corpus are very good, outperforming the ones obtained by other classification systems for some genres.", "Moreover, when mean results per experiment are considered, the SVD generated dimensional representation always achieves the best results, which seems to suggest that it is a very robust and suitable representation for coreference mention-pairs.", "As future work, we plan to experiment with some other kind of multi-classifer systems and basic classifiers such as SVM.", "It is important to note that the approach may be applied to corpora in other languages as well.", "Country, UPV/EHU, ikerketaren arloko errektore- ordetza / Vicerrectorado de Investigacio\u0301n."]}, {"summaries": ["For projection, we use a procedure based on sentence and word alignment as calculated by a standard tool (GIZA++) that was trained on corpora of moderate size.", "We apply a direct projection algorithm on a multi-genre and multilingual corpus (English, German, Russian) to automatically produce coreference annotations for two target languages without exploiting any linguistic knowledge of the languages."], "paperId": "W15-3403", "paperName": "2011-3", "document": ["Common technologies for automatic coreference resolution require either a language-specific rule set or large collections of manually annotated data, which is typically limited to newswire texts in major languages.", "This makes it difficult to develop coreference resolvers for a large number of the so-called low-resourced languages.", "We apply a direct projection algorithm on a multi-genre and multilingual corpus (English, German, Russian) to automatically produce coreference annotations for two target languages without exploiting any linguistic knowledge of the languages.", "Our evaluation of the projected annotations shows promising results, and the error analysis reveals structural differences of referring expressions and coreference chains for the three languages, which can now be targeted with more linguistically-informed projection algorithms.", "Coreference resolution requires relatively expensive resources, usually in terms of manual annotation.", "To alleviate this problem for low-resourced languages, techniques of annotation projection can be applied.", "In this paper, we report on experiments with projecting nominal coreference chains across bilingual corpora.", "Our goal is to see how well a knowledge-lean projection algorithm works for two relatively similar languages (English-German) and for less similar languages (English-Russian).", "Furthermore, we are interested in differences incurred by the text genre and therefore use three different genres: argumentative newspaper articles, narratives, and medicine instruction leaflets.", "Our general aim is to explore the limitations of a knowledge-lean approach to the problem, so that it is easy to generalize to other low-resourced languages.", "For the annotation of the corpus, we created common annotation guidelines that make few assumptions on the structural features of the target languages.", "We used the guidelines to annotate texts of the three genres in the three languages, and provide results on inter-annotator agreement (see Section 3).", "For projection, we use a procedure based on sentence and word alignment as calculated by a standard tool (GIZA++) that was trained on corpora of moderate size.", "Thus at this point we deliberately do not apply linguistic knowledge on the languages involved.", "The experiments and results are described in Section 4.", "We present a qualitative error analysis showing that a number of structural divergences are responsible for many of the problems; this suggests that limited syntactic knowledge can be helpful for improving performance in follow-up work.", "A projection approach is used to automatically transfer different types of linguistic annotation from one language to another.", "The idea of mapping from well-studied languages to low-resourced languages was initially introduced in the work of  CITE-p-15-3-8 , who studied the induction of PoS and NE taggers, NP chunkers and morphological analyzers for different languages using annotation projection.", "Thereafter, the technique has been used for a variety of tasks, including PoS tagging and syntactic parsing ( CITE-p-15-1-7 ,  CITE-p-15-1-15 ,  CITE-p-15-3-4 ), semantic role labelling ( CITE-p-15-1-16 ), sentiment analysis ( CITE-p-15-1-11 ), mention detection ( CITE-p-15-3-9 ), or named-entity recognition ( CITE-p-15-1-2 ).", "Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 14\u201322, Beijing, China, July 30, 2015.", "2015c Association for Computational Linguistics", "To our knoweldge, the first application to coreference is due to Harabagiu and Maiorano (2000), who experimented with manually projecting coreference chains from English to Romanian using a translated parallel corpus.", "They showed that a coreference resolver trained on a parallel corpus can achieve better results than one trained on monolingual data.", "Then, Postolache and colleagues (2006) used automatic word alignment to project coreference annotations for the same data.", "Their goal was to achieve high precision, and thus they discarded from projection those referring expressions (henceforth: REs) whose syntactic heads were not properly aligned.", "Their results indeed show high precision (over 95%), but considerably lower recall (around 70%).", "Mitkov and  CITE-p-15-1-12  performed anaphora resolution using projection on a parallel English-French corpus, which lead to an improvement in the success rate of roughly 4% for both English and French.", "( CITE-p-15-3-1 ) used cross-lingual projection to improve the detection of coreferent named entities with the help of English-Arabic translations, and they reported better results than a monolingual resolver could achieve.", "( CITE-p-15-3-0 ) used translation-based projection to train a coreference resolver, and achieved around 90% of the average F-scores of a supervised resolver in experiments with Spanish and Italian using few resources (only a mention extractor) for the target languages.", "Our corpus consists of 38 parallel texts in English, German and Russian, belonging to three genres: newswire articles (7 texts per language), short stories (3 texts per language), and medicine instruction leaflets (4 per language, only English-German) 1 .", "This choice is motivated by (i) the common observation that narrative texts are easier to process for coreference, (ii) the fact that news text is important for many applications, and (iii) the consideration of medical leaflets representing a somewhat \u201cexotic\u201d genre that exhibits many differences to the other two.", "Corpus statistics are shown in Table 1.", "The stories contain more REs than the newswire texts, and the coreference chains of the stories tend to be much longer.", "Usually, coreference annotation guidelines have been designed with one target language in mind.", "In contrast, our goal was to have common guidelines for the three languages, in order to (i) obtain uniform nominal coreference annotations in our corpus (supporting the projection task), and (ii) facilitate extension to further languages.", "Regarding English, our guidelines are of similar length and quite compatible with the scheme used for OntoNotes - the largest annotated coreference corpus for the English language ( CITE-p-15-1-6 ).", "One exception is that we handle only NPs and do not annotate verbs that are coreferent with NPs.", "Our guidelines borrow many decisions from the (relatively language-neutral) Potsdam Coreference Scheme (PoCoS) (Krasavina and Chiarcos, 2007), and we also considered the recently developed guidelines for thr English-German parallel corpus ParCor ( CITE-p-15-1-4 ).", "But it considers only pairwise annotation of anaphoric pronouns and their antecedents, whereas we annotate all REs appearing in a coreference chain (i.e. that are mentioned in the text at least twice).", "For the time being, our annotation is restricted to the referential identity; we thus exclude cases of \u2018bridging\u2019 (also called \u2019indirect anaphora\u2019) or near-identity.", "The following types of REs are considered as markables: full NPs, proper names, and pronouns (personal, demonstrative, relative, reflexive, and pronominal adverbs).", "As in OntoNotes, generic nouns can corefer with definite full NPs or pronouns, but not with other generic nouns.", "In case of English nominal pre-modifiers, we only annotate a nominal premodifier if it can refer to a named entity (the [US] 1 politicians) or is an independent noun in the Genitive form ([creditor\u2019s] 1 choice); in all other cases, nominal premodifiers are not annotated as separate markables (e.g., [bank account]).", "When annotators identify a markable, they also record its RE type from an attribute menu.", "The markable span includes the syntactic head of the NP and all its modifiers, except for dependent relative clauses (because relative pronouns are treated as separate markables).", "As a divergence from OntoNotes, they have a separate relation for appositions, whereas we only include them in the head NP markable.", "Technically, we used the MMAX- 2 coreference annotation tool 2 , and the corpus was tokenized and split into sentences using the Europarl preprocessing tools 3 .", "The English-German corpus was annotated by two lightly-trained independent annotators - students of linguistics.", "(For Russian, we had only one annotator available, therefore the agreement study will be done later.)", "For markables, we computed the inter-annotator agreement using Cohen\u2019s kappa in two settings: binary overlap and proportional overlap.", "For binary overlap, we consider two markables as \u201cagreed\u201d if they overlap by at least one token; proportional overlap measures the extent to which annotators agree on the identification of spans (number of overlapping tokens).", "For the coreference annotation, we computed MUC scores with strict mention matching.", "Automatic sentence and word alignment.", "We aligned the source and target parts of the corpus at the sentence level using the HunAlign sentence aligner ( CITE-p-15-3-6 ) and its wrapper LF Aligner 4 , which already includes alignment dictionaries for the required language pairs.", "Word alignment was performed with GIZA++ ( CITE-p-15-1-14 ) using the standard settings.", "Before the alignment, all texts in the corpus were tokenized and lowercased using the Europarl preprocessing tools.", "The word aligner was trained on a collection of bilingual newswire text from our source given above, preprocessed in the same way as descibed above.", "The training set consists of around 200 000 parallel sentences for English-German, and 170 000 for English-Russian.", "We computed both bidirectional alignments and the intersection of source-target / target-source alignments.", "(Annotation projection is often done with intersective alignments, as they provide higher precision than bidirectional alignments.)", "For English-German, we evaluated our word alignment against a set of 1000 manually annotated parallel sentences made available by S. Pad\u00f3 5 .", "For English-Russian, we are not aware of any similar gold alignments and thus did not evaluate.", "Results are given in Table 4.", "Following ( CITE-p-15-1-17 ), we evaluated only the resulting intersective alignments.", "We compared our results to those of ( CITE-p-15-1-17 ) and ( CITE-p-15-3-2 ), who used the English-German part of the Europarl dataset.", "Our results are somewhat lower, probably due to the much smaller training set.", "To simplify subsequent processing, we converted the corpus annotations into the CoNLL table format 6 using discoursegraphs converter (Neumann, 2015).", "Extraction of REs and transfer of coreference chains.", "For each RE in the source language we extract the corresponding RE in the target language, together with its coreference set number.", "Following the approach of  CITE-p-15-1-18 , for each word span representing an RE in the source language, we extract the corresponding set of aligned words in the target language.", "The resulting target RE is the span between the first and the last extracted word, and it belongs to the same set as the source RE.", "Table 5 shows the number of REs and coreference chains projected through word alignment (from English).", "We evaluate both the quality of the identification of mentions and the extraction of coreference chains using the CoNLL scorer 7 .", "1.", "Evaluation of the identification of mentions.", "We compute the scores for the identification of mentions using the strict mention matching as in the CoNLL-2011 ( CITE-p-15-1-19 ) and CONLL-2012 shared tasks (Pradhan et al., 2012), so that we score only those projected markable spans that are exactly the same as the gold ones.", "The values for English-German and English-Russian are given in Table 6 as mentions.", "2.", "Evaluation of coreference chains", "We evaluate all the projected coreference chains against gold chains using the standard coreference evaluation metrics MUC ( CITE-p-15-3-7 ), CEAF ( CITE-p-15-1-10 ) and B 3 ( CITE-p-15-1-0 ) to get complete performance characteristics.", "We also use strict matching as in the evaluation of the identification of mentions and evaluate the projected markables against all the markables of the gold standard.", "These scores depend on the identification of mentions evaluated in the previous step.", "We report the micro-averaged Precision, Recall and F-1 scores in Table 6.", "In addition, Figure 1 shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B 3 ) for both language pairs as boxplots.", "3.", "Evaluation of coreference chains with minimal spans", "Finally, we evaluate using just minimal spans of the REs, i.e., syntactic heads.", "This indicates how well the REs can be projected, not punishing the algorithm for detecting only partially correct REs.", "We manually annotated syntactic heads of the gold and projected REs.", "Following the approach of Postolache et al. (2006), we select the leftmost noun, pronoun or numeral as head; otherwise, the RE is discarded.", "Results are given in Table 6 with the tag \u2018min\u2019.", "From a formal viewpoint, there are three categories of projection problems:", "1.", "An RE is present in both source and target text, but it is not projected correctly, or not at all, on the grounds of mistakes in the word alignment phase.", "2.", "An RE is present in the source text and correctly projected into the target text, but it does not show up in the gold standard, because the target language text does not have a corresponding RE pair (the target language does not reproduce the complete chain of the source).", "3.", "An RE in the gold standard is not present in the target text and therefore can not be projected (the dual problem to (2): the source text does not have an RE pair that would correspond to one in the target text).", "The number of errors caused by wrong word alignment (1) can be estimated on the basis of the alignment evaluation (Section 4.1), albeit only for the English-German language pair; due to the lack of resources, this is not possible for English-Russian.", "Problems (2) and (3) are the more interesting ones for a qualitative error analysis.", "For this purpose, we visualized the projected files and the gold standard using the coreference module of the ICARUS corpus analysis platform ( CITE-p-15-1-3 ).", "50% of the data was randomly selected for the detailed analysis, and we determined the most frequent projection errors and categorized them into three different groups.", "Thereafter, we tried to verify our resulting hypotheses about variation in pronominal coreference in the three languages using a larger external corpus: InterCorp 8 (C\u030cerm\u00e1k and Rosen, 2012) offers an online interface for searching parallel corpora in different languages and subcorpora.", "We performed both monolingual and multilingual queries (e.g. querying one side of a parallel corpus vs. querying parallel data).", "Further, we were interested in comparing our findings to available studies on multilingual nominal coreference in Contrastive Linguistics.", "However, the only work we found on this topic is a comparative study of nominal referring expressions for newswire texts in English and German ( CITE-p-15-1-9 ).", "In our data, the problematic cases are those where the source language (SL) referring expression is missing or reformulated in the target text (TL), and therefore is not being projected.", "We identified three categories of errors caused by structural differences among the three languages:", "Morphological differences.", "These are cases of German contractions and compound nouns.", "For example, as in the case of policy towards [minorities] 1 and [Minderheiten]politik, the SL markable is not present in the TL as a separate unit, since we cannot split compound nouns and mark only a part.", "Also, cases like zum Bahnhof short for zu dem Bahnhof (\u2018to the station\u2019) cause errors in the identification of spans, because we do not annotate prepositions as parts of markables on the English side.", "However, such cases are frequent in the German data, where, in general, the prepositions an, bei, in, von, zu can be contracted with subsequent determiners in written text.", "Our corpus study has shown that for the preposition zu (\u2018to\u2019) the frequency of the contraction is 16 times higher than for the full form (InterCorp, measured in items per million (henceforth  MATH-w-11-7-0-176 ).", "Differences in NP syntax.", "1: The use of articles.", "Some NPs are more frequently used with a definite article in German than in English, which resulted in the misidentification of spans.", "According to  CITE-p-15-1-9 , English allows the use of nouns with zero article more frequently than German.", "This is true for both singular and plural nouns.", "In our guidelines, nouns with zero article can only be linked to anaphoric pronouns (if any), but not between each other (like in OntoNotes).", "This resulted in mismatching chains: English NPs with zero article do not form chains and therefore cannot be projected, while the same NPs actually form a chain in German.", "(1) a. Lastly, the G-20 could also help drive momentum on climate change.", "<...> We also have to find", "a way to provide funding for adaptation and mitigation - to protect people from the impact of climate change and enable economies to grow while holding down pollution levels - while guarding against trade protection in the name of climate change mitigation.", "b. Schlie\u00dflich k\u00f6nnten die G-20 auch f\u00fcr neue Impulse im Bereich [des Klimawandels] 1 sorgen.", "Ebenso m\u00fcssen wir einen Weg finden, finanzielle Mittel f\u00fcr die Anpassung an [den Klimawandel] 1 sowie dessen Eind\u00e4mmung bereitzustellen - um die Menschen zu sch\u00fctzen und den \u00d6konomien Wachstum zu erm\u00f6glichen, aber den Grad der Umweltverschmutzung trotzdem in Grenzen zu halten.", "Au\u00dferdem gilt es, sich vor handelspolitis- chen Schutzma\u00dfnahmen im Namen der Eind\u00e4m- mung [des Klimawandels] 1 zu h\u00fcten .", "The query of InterCorp data has shown that German exhibits a higher number of NPs with definite article (57.928,55 i.p.m.) compared to English (31.405,22 i.p.m.).", "We also noticed that article use with named entities can vary in both languages (for example, the English Hamas corresponds to the German die Hamas).", "However, our corpus queries did not show any regularities yet; this issue requires a more detailed study regarding the types of named entities (which we assume to be the reason for the different use of articles).", "In the case of Russian, the absence of articles led to better results in the identification of REs, since in general, shorter spans increase the chance for a perfect alignment.", "2: The use of reflexive pronouns.", "According to our annotation scheme, we annotated reflexive pronouns only when they are independent constituents (rather than verb particles), but we observe differences in the use of these pronouns for the three languages, so that in most cases these are non-parallel.", "These differences have to do with the form and distribution of reflexive pronouns.", "In English, we only have -self to express reflexivity, while in German and Russian a wider range of reflexives can be used.", "In German and Russian, it is possible to use more than one reflexive in a sentence to emphasize the action, which is not possible in English.", "As a result, there is less reflexives to be transferred from English to the target (German and Russian) sides of the corpus which led to errors in the projection.", "3: Pre-and post-modification.", "In general, we noticed that German NPs allow more complicated premodification than English and Russian.", "According to  CITE-p-15-1-9 , English tends to postmodification, while German is less restrictive with pre-modification.", "These variations result in syntactical differences in markables and in non-parallelism.", "Regarding the participial constructions, one of the complications is that in German, they occur only in preposition, while in English and Russian they can be placed in both pre-and postposition.", "(2) a. Pakistan needs international help to bring hope to [the young people] 1 [who] 1 live there.", "b. Pakistan braucht internationale Hilfe, um [den dort lebenden jungen Menschen] 1 Hoffnung zu bringen.", "Non-equivalences in translation.", "The following cases of non-parallelism resulted in projection errors in our dataset; however, we could not find enough evidence to characterize them as systematic.", "\u2022 Personal pronouns vs. indefinite pronouns.", "(3) a. [It] 1 was pursuing a two-pronged strategy.", "b. [Man] verfolgte eine Doppelstrate- gie.", "(\u2018One followed a two-pronged strategy.\u2019)", "The German indefinite pronoun man is the target of the projected annotations, but it is not a markable according to our guidelines: it is non-referring and thus unable to participate in RE chains.", "\u2022 Possessivesessive NPsNPsin vsthe.", "adjectivesSL (for example.", "Some,pos-the government of [India] 1 ) can be expressed through adjectives in the TL (die [indis- che] Regierung or indijskoe pravitel\u2019stvo ([indii\u0306skoe] pravitel~stvo)) and therefore are no markables.", "\u2022 Determinerssonal pronounsvs.inpossessiveEnglish canpronounsbe translated.", "Per- as articles in German (for example, [its] 1 broader goal = das weiter gefasste Ziel), so that the source RE has no correspondent in the TL.", "For Russian, in this case a possessive form of a reflexive pronoun svoj (svoi\u0306) can be used, or the possessive pronoun can be omitted.", "\u2022 Relativespond toclausesparticiplein oneconstructionslanguage canor PPscorre-in another.", "Examples: a. [a fat lady] 1 [who] 1 wore a fur around her neck b. [eine dicke Dame mit einer Pelzstola] 1 (\u2018a fat lady with a a fur\u2019)", "According to Table 6 and Figure 1, we see that newswire texts get the lowest scores, the reason most likely being the more complicated NPs.", "In setting 2 (evaluation of minimal spans), both newswire texts and stories obtain closer F1-scores, but the stories still have better precision scores.", "The medicine instruction leaflets in setting 2 have the worst results, and we observe lower improvement for precision between two settings compared to the newswire texts.", "This indicates that the quality of coreference resolution for medical texts depends to a higher degree on the coreference relations, than on the identification of mentions.", "In these texts, we frequently find borderline cases of non-/reference, when dieseases, parts of the body, etc. are being mentioned.", "The most closely related work is the approach of ( CITE-p-15-1-18 ), but some differences are noteworthy.", "In contrast to Postolache and colleagues, we do not focus on maximising precision; instead, our goal is to assess how well projection can work for all the annotations.", "In general, we use neither language-dependent software nor any additional linguistic information about the target language in the coreference projection and evaluation.", "Postolache et al., in contrast, applied a dedicated Romanian-English word aligner 9 (which achieves an F-score of 83.3% compared to our 66.05% of the language-independent GIZA++) and used special rules that rely upon the POS information and syntactic heads to produce their annotations, and then discarded the incorrectly projected ones (we used such rules only in the evaluation of the projected heads of REs).", "In our case, we use all REs to evaluate the spans of the projected annotations and the resulting coreference chains.", "Comparing our evaluation to Postolache\u2019s evaluation of all REs, we can see that our results yield a higher MUC precision for all of the genres (average 68.0 for English-German, 82.1 for English-Russian vs. 52.3 for English-Romanian), but a lower recall for both languages (45.8/62.6 vs. 82.04), which results in different F-measure (Postolache et al. obtained an average F1 of 63.9 compared to our F1 of 54.6 for German and 71.0 for Russian).", "This can be explained by the lower quality of our automatic English-German alignments compared to the English-Romanian; the Russian REs were extracted slightly more accurately due to the structural differences in NPs.", "We also observed different scores for newswire texts, stories and medical leaflets, while Postolache et al. only used texts of one genre and in fact one author (different chapters of the same fiction book).", "Keeping these different parameters in mind, in order to compare our results in a fair way, we evaluated the RE heads following the same rules to extract minimal spans of the projected REs, and evaluated them against manually annotated heads in the gold standard.", "In this setting, we obtained higher precision than in the previous setting, and in comparison to Postolache et al. (English-Romanian, avg.", "F1 = 80.5), our results are somewhat lower for English-German (avg.", "F1 = 74.1) and slightly better for English-Russian (avg.", "F1 = 81.3), which we attribute to the overall more difficult (and therefore more generalizable) projection scenario in our approach.", "The goal of this study was to explore to what extent the coreference projection task can be tackled with a decidedly \u201clight weight\u201d approach.", "In contrast to earlier work, we used a well-known, standard word alignment tool trained on a corpus of moderate size.", "Furthermore, we deliberately worked with projecting English annotations to two relatively different languages, Russian and German, in order to study the limitations of the approach.", "In order to be as \u201cgeneralizable\u201d as possible (especially for other low-resourced languages), we work on the basis of common, relatively lean, annotation guidelines for coreference, which make few assumptions on the specifics of the languages considered here.", "We compared our results quantitatively to the most closely related work and argued that they are competitive, in particular because our task setting is more target-language-neutral, we used three languages rather than two, and we worked on three different genres of text.", "Our qualitative error analysis showed that problems are due to a set of structural differences of NPs in the three languages.", "Having completed this \u201clightweight\u201d study, we will now move forward by introducing limited syntactic knowledge of the languages involved (NP chunking) and explore how much performance can be gained in that way.", "Still, our emphasis remains on devising procedures that are generalizable to other low-resourced languages, so we will do these extensions in small steps only.", "Our annotation guidelines and other material will be made available via our website http://www.ling.uni-potsdam.de/acl-lab/."]}], "queries": ["Learning Framework", "Markable Identification"]}, {"setId": "CoNLL-2012", "papers": [{"summaries": ["The models were trained with logistic regression optimized by stochastic gradient descent.", "We introduced a system for coreference resolution via projection for German and Russian."], "paperId": "W17-1508", "paperName": "2012-1", "document": ["The paper describes the system for coreference resolution in German and Russian, trained exclusively on coreference relations projected through a parallel corpus from English.", "The resolver operates on the level of deep syntax and makes use of multiple specialized models.", "It achieves 32 and 22 points in terms of CoNLL score for Russian and German, respectively.", "Analysis of the evaluation results show that the resolver for Russian is able to preserve 66% of the English resolver\u2019s quality in terms of CoNLL score.", "Projection techniques in parallel corpora are a popular choice to obtain annotation of various linguistic phenomena in a resource-poor language.", "No tools or gold manual labels are required for this language.", "Instead, far more easily available parallel corpora are used as a means to transfer the labels to this language from a language, for which such a tool or manual annotation exists.", "This paper presents a system submitted to the closed track of the shared task collocated with the Workshop on Coreference Resolution Beyond OntoNotes ( CITE-p-13-3-3 ).", "1 The task was to build coreference resolution systems for German and Russian without coreference-annotated training data in these languages.", "The only allowed coreference-annotated training data was the English part of the OntoNotes corpus ( CITE-p-13-3-15 ).", "Alternatively, any publicly available resolution tool trained on this corpora could be employed.", "We adopted and slightly modified an approach previously used by de Souza and Ora\u0306san (2011) and  CITE-p-13-3-6 .", "Parallel English-German and English-Russian corpora are used to project coreference links that had been automatically resolved on the English side of the corpora.", "The projected links then serve as input data for training a resolver.", "Unlike the previous works, our coreference resolution system operates on a level of deep syntax.", "The original surface representation of coreference thus must be transferred to this level.", "Likewise, coreference relations found by our system must be in the end transformed back to the surface representation, so that they can be evaluated in accordance with the task\u2019s requirements.", "According to the official results, we were the only participating team.", "Our system achieved 29.40 points and 30.94 points of CoNLL score for German and Russian portion of the official evaluation dataset, respectively.", "The paper is structured as follows.", "After introducing related works in Section 2, the paper continues with description of the system and its three main stages (Section 3).", "Section 4 lists the training and testing data to enable evaluation of the proposed system in Section 5.", "In Section 6, the resolver is analyzed using two different methods.", "Approaches of cross-lingual projection have received attention with the advent of parallel corpora.", "They are usually aimed to bridge the gap of missing resources in the target language.", "So far, they have been quite successfully applied to part-of-speech tagging ( CITE-p-13-5-4 ), syntactic parsing ( CITE-p-13-3-4 ), semantic role labeling ( CITE-p-13-3-11 ), opinion mining ( CITE-p-13-1-0 ), etc.", "Coreference resolution is no exception in this respect.", "Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes ( CITE-p-13-3-3 ), co-located with EACL 2017, pages 56\u201364, Valencia, Spain, April 4, 2017.", "2017c Association for Computational Linguistics", "Coreference projection is generally approached in two ways.", "They differ in how they obtain the translation to the language for which a coreference resolver exits.", "The first approach applies a machine-translation service to create synthetic data in this language.", "This usually happens at test times on previously unseen texts.", "Such approach was used by Rahman and  CITE-p-13-3-19  on Spanish and Italian, and by  CITE-p-13-3-10  on Polish.", "The other approach, which we employ in this work, takes advantage of the human-translated parallel corpus of the two languages.", "Unlike the first approach, the translation must be provided already in train time.", " CITE-p-13-3-13  followed this approach using an English-Romanian corpus.", "They projected manually annotated coreference, which was then postprocessed by linguists to acquire high quality annotation in Romanian.", "de Souza and Ora\u0306san (2011) applied projection in a parallel English-Portuguese corpus to build a resolver for Portuguese.", "Our work practically follows this schema, differing in some design details (e.g., using specialized models, resolution on a level of deep syntax).", " CITE-p-13-3-6  extended this approach by learning coreference with a specific type of regularization at the end.", "Their gains over the standard projection come from ability of their method to recover links missing due to projection over inaccurate alignment.", "Our system for coreference resolution is an example of the projection in parallel corpus.", "It requires a corpus of parallel sentences in a source (English) and a target language (German and Russian).", "The procedure consists of three stages illustrated in Figure 1.", "First, coreference links on the source-language side of the corpus are automatically resolved (see Section 3.1).", "The acquired links are then projected to the target-language side (Section 3.2).", "Finally, the target-language side enriched with the projected links is used as a training data to build a coreference resolver (Section 3.3).", "The source-language side of the parallel corpus must get labeled with coreference.", "In our case, the English side of the parallel corpus already contained annotation of coreference provided by the shared task\u2019s organizers.", "The annotation is obtained by Berkeley Entity Resolution system (Durrett and Klein, 2014), trained on the English section of OntoNotes 5.0 ( CITE-p-13-3-15 ).", "Although Berkeley system is a state-of-the-art performing coreference resolver, we found that it rarely addresses relative and demonstrative pronouns.", "To label coreference for relative pronouns, we introduced a module from the Treex framework 2 that employs a simple heuristics based on syntactic trees.", "Coreference of demonstratives has not been further resolved.", "The second stage the proposed schema is to project coreference relations from the source-language to the target-language side of the parallel corpus.", "Specifically, we make use of word-level alignment, which allows for potentially more accurate projection.", "As the parallel data provided for the task are aligned only on the sentence level, word alignment must be acquired on our own.", "For this purpose, we used GIZA++ ( CITE-p-13-3-9 ) a tool particularly popular in the community of statistical machine translation.", "Even though GIZA++ implements a fully unsupervised approach, which allows for easy extension of the training data with raw parallel texts, it did not prove to be useful for us.", "We thus obtained word alignment for both the language pairs by running the tool solely on the parallel corpora coming from the organizers.", "Since both German and Russian are morphologically rich languages, we expected word alignment to work better on lemmatized texts.", "We applied TreeTagger ( CITE-p-13-3-18 ), and MATE tools ( CITE-p-13-1-3 ) for lemmatization in Russian and German, respectively.", "For robustness, also English texts were preprocessed with a similar procedure, namely a rule-based lemmatization available as a module in the Treex framework.", "Based on the word alignment, the projection itself works as shown in Figure 1.", "We project mention spans along with its entity identifiers, which are shared among the cluster of coreferential mentions.", "Only such a mention is projected, whose counterpart forms a consecutive sequence of tokens in the target-language text.", "In practice, this approach succeeds in projecting around 90% of mentions.", "At this point, projected links are ready to serve as training data for a coreference resolver.", "We make use of an updated version of the already existing resolver implemented within the Treex framework, which operates on a level of deep syntax.", "All the texts must thus be analyzed and the projected mentions must be transferred up to this level before being used for training.", "Analysis up to the tectogrammatical layer.", "Treex coreference resolver operates on a level of deep syntax, in Prague theory ( CITE-p-13-5-1 ) called tectogrammatical layer.", "On this layer, a sentence is represented as a dependency tree.", "Compared to a standard surface dependency tree, the tectogrammatical one is more compact as it consists only of content words (see Figure 1).", "In addition, several types of ellipsis can be reconstructed in the tree, e.g. pro-drops.", "To transform a text in a target language from a surface form to a tectogrammatical representation, we processed it with the following pipelines:", "German texts are processed with the MATE tools pipeline ( CITE-p-13-1-3 ) that includes lemmatization, part-of-speech tagging, and transition-based dependency parsing ( CITE-p-13-1-5 ,  CITE-p-13-3-19 ).", "The surface dependency tree is then converted to the Prague style of annotation using a converter from the HamleDT project ( CITE-p-13-5-7 ).", "Transformation to tectogrammatics is then performed by a general Treex pipeline, with some language-dependent adjustments.", "Russian texts are being parsed directly to the Prague style of surface dependency tree.", "We trained a UDPipe tool ( CITE-p-13-5-2 ) on data from SynTagRus corpus ( CITE-p-13-1-4 ) converted to the Prague style within the HamleDT project.", "5 Although UDPipe trained on this data is able to lemmatize, we used lemmas produced by TreeTagger instead, as they seemed to be of better quality.", "In the same fashion as for German, tectogrammatical tree is built from the surface dependency tree using the Treex pipeline adjusted to Russian.", "We also included named entity recognition, namely NameTag tool ( CITE-p-13-5-3 ), to the pipeline.", "We had trained it on an extended version of the Persons-1000 collection ( CITE-p-13-3-8 ) and named entity annotation of the NoSta-D corpus ( CITE-p-13-1-2 ) for Russian and German, respectively.", "Transfer of mentions from the surface and back.", "On the tectogrammatical layer, a coreference link always connects two nodes that represent heads of the mentions.", "Tectogrammatics does not specify a span of the mention, though.", "The mention usually spans over the whole subtree, except for some notable cases.", "For instance, an antecedent of a relative pronoun does not include the relative clause itself in its span, even though the clause belongs to a subtree of the antecedent.", "The transfer from the surface to the tectogrammatics is easy \u2013 a head of the mention must be found.", "We use the dependency structure of a tectogrammatical tree for this and out of all nodes representing nouns or pronouns contained in the mention we pick the one that is closest to the root of the tree.", "In the opposite direction, we consider the whole tectogrammatical subtree of a coreferential node.", "As mentions observed in the datasets rarely include a dependent clause, we rather exclude all such clauses.", "We skip possible trailing punctuation and finally, we mark the first and the last token of such selection as boundaries of the mention.", "Due to strict rules to find a mention span and possibly scrambled syntactic parses, this transfer is prone to errors (see Section 6).", "Specialized models and features.", "Treex resolver implements a mention-ranking approach ( CITE-p-13-3-1 ).", "In other words, every candidate mention forms an instance, aggregating all antecedent candidates from a predefined window of a surrounding context.", "The antecedent candidates are ranked and the one with the highest score is marked as the antecedent.", "Moreover, a dummy antecedent candidate is added.", "Highest score for the dummy antecedent implies that the candidate mention is not anaphoric, in fact.", "In detail, the resolver consists of multiple models, each of them focused on a specific mention type, e.g., relative pronouns, demonstrative pronouns, or noun phrases.", "It makes possible to use different windows and different features for each of the types.", "Personal and possessive pronouns are addressed jointly by two models: a model for personal and possessive pronouns in third person and a model for these pronouns in other persons (in the following denoted as PP3 and PPo pronouns, respectively).", "Features exploit information collected during the analysis to the tectogrammatical layer.", "As seen in the table, our models are trained using two kinds", "Mention type DE RU Window Featset X X curr5sentprev, precedingsents NP NP PP3 X X 1 prev sent PPo X X X X curr sent, preceding general demonstrative reflexive X X curr sent, all reflexive possessive \u00d7 X relative X X curr sent, preceding", "The models were trained with logistic regression optimized by stochastic gradient descent.", "We varied different values of hyperparameters (e.g., number of passes over data, L1/L2 regularization) and picked the setting best performing on the De- vAuto set (see Section 4).", "The learning method is implemented in the Vowpal Wabbit toolkit.", "Raw datasets without manual annotation of coreference are used to train the pipeline described in Section 3.", "In contrast, manually annotated datasets are reserved exclusively for evaluation purposes.", "Table 2 shows some basic statistics of the datasets.", "We refer to each dataset by its label, which consists of two parts.", "The first part denotes the main purpose of the dataset: Train is used for training, Dev for development testing, and Eval for blind evaluation testing.", "The second part indicates the origin of the coreference annotation contained in the dataset: Auto denotes the projected automatic annotation, Off is the official manual annotation provided by the task\u2019s organizers, and Add denotes the additional dataset annotated by the authors of this paper.", "Raw data.", "We employed the parallel corpora provided by the task\u2019s organizers for building the resolver.", "Both the English-German and English-Russian corpora come from the News- Commentary11 collection ( CITE-p-13-5-5 ).", "The datasets were provided in a tokenized sentence-aligned format.", "We split both corpora into two parts: TrainAuto and DevAuto.", "While the former is used for training the models, the latter serves to pick the best values of the learning method\u2019s hyperparameters (see Section 3.3).", "Coreference-annotated data.", "For evaluation purposes, we used two datasets manually annotated with coreference: DevOff and DevAdd.", "Except for these datasets, a dataset for the final evaluation (EvalOff) of the shared task was provided by the organizer.", "However, the coreference annotation of this dataset has not been published.", "Similarly to the raw data, DevOff has been provided by the task\u2019s organizers.", "In fact, both in German and Russian it is represented by a single monolingual document, presumably coming from the News-Commentary11 collection.", "DevAdd dataset consists of the same five documents randomly selected from both the English-German and English-Russian parallel corpora so that none of these are included in TrainAuto.", "Coreference relations were annotated on all the three language sides.", "The Russian and English sides were labelled by one of this paper\u2019s coauthors, who speaks native Russian and fluent English, and has long experience of annotating anaphoric relations.", "The German side was split among three annotators and their outputs were revised by the annotator of the Russian and English part to reach higher consistency.", "They all followed the annotation guideline published by the organizers.", "8 The reason for creating additional annotated data is that the DevOff set consists only of a thousand words per language, which we found insufficient to reliably assess quality of designed systems.", "The English side was labelled to allow for assessing the quality of the projection pipeline over its stages (see Section 6).", "Let us show some notable properties of the German and Russian evaluation data.", "Table 2 highlights that the DevAdd sets expectedly contain five times more words than their DevOff counterparts.", "However, the number of sentences is six times bigger.", "This may affect a proportion of individual mention types.", "Table 3 gives a detailed picture of candidate and anaphoric mentions\u2019 counts.", "Whereas Russian anaphoric NPs account for 75% of all the anaphoric mentions in DevOff, it is only 50% in DevAdd.", "The disproportion appears also between the German datasets.", "Finally, some of the mention types appear rarely in the DevOff sets.", "It especially holds for the Russian DevOff containing a lack of reflexive, relative and PPo pronouns.", "Conversely, some of the even well-populated types are rarely or never anaphoric (e.g., German demonstrative, reflexive and PPo pronouns).", "For both German and Russian, we submitted a single system to the shared task.", "Both the systems fulfill the requirements set on the closed track of the task.", "To build them we exploited the parallel English-German and English-Russian corpora selected from the News-Commentary11 collection by the task\u2019s organizers.", "Metrics.", "We present the results in terms of four standard coreference measures: MUC ( CITE-p-13-5-6 ), B 3 ( CITE-p-13-1-1 ), CEAFe ( CITE-p-13-3-5 ) and the CoNLL score ( CITE-p-13-3-16 ).", "The CoNLL score is an average of F-scores of the previous three measures.", "It was the main score of some previous coreference-related shared tasks, e.g., CoNLL 2012 ( CITE-p-13-3-14 ), and it remains so for the CORBON 2017 Shared task.", "Results.", "In Table 4, we report the results of evaluating the submitted systems.", "Comparison across languages shows very similar performance on the DevOff set.", "However, evaluation on the larger De- vAdd set suggests the Russian resolver performs better.", "Scores on the EvalOff dataset confirms higher quality of the Russian resolver, however, the gap is not so big.", "As the latter dataset is the largest, these results can be considered the most reliable.", "We conducted two additional experiments to learn more about the properties of the projection system.", "The first experiment investigates the impact of models for individual mention types.", "The second experiment, in contrast, should tell us more about the quality of the system over its stages.", "Model ablations.", "We conducted a model ablation experiment to shed more light on the model quality and difference between the two evaluation datasets.", "We repeated the same evaluation, however, each time with a model for a specified mention type left out.", "Results in Table 5 show that models for PP3 pronouns and NPs are the most valuable.", "Better performance of the Russian resolver on DevAdd seems to partly result from a decent model for reflexive possessives, which do not exist in German.", "Other observations accord with what we highlighted above after inspecting datasets\u2019 statistics in Table 3.", "There is a big disproportion in score between the two datasets after the model for NPs is removed.", "This may be a consequence of different ratios of anaphoric NPs to all the anaphoric mentions.", "Multiple models seem to have marginal, zero, or even negative impact on the final performance.", "\u2022 low frequency of the mention type in DevOff (e.g., Russian relative and PPo pronouns);", "\u2022 low frequency of its anaphoric occurrences in the dataset (e.g., all demonstrative pronouns, German reflexive and PPo pronouns)", "\u2022 the model learned to label most candidates as non-anaphoric (e.g. German demonstrative and reflexive pronouns)", "Performance over projection stages.", "The final performance about 20-30 points seems to be much worse than the CoNLL scores over 60 points observed at the CoNLL 2012 shared task for English.", "Is coreference resolution in German and Russian so difficult or the projection system deteriorates as it proceeds over its stages?", "To answer these questions, we evaluated the output of four stages of the projection CR system.", "First, we scored the original automatic coreference annotation provided by the Berkeley resolver and the Treex resolver for relative pronouns.", "This tells us the performance of English CR, which should be comparable with the CoNLL shared task systems.", "Second, English coreference projected to the target language was evaluated.", "It should quantify the effect of cross-lingual projection of coreference.", "Third, all projected coreference relations were transferred to the tectogrammatical layer and back to the surface.", "This should find the price we pay for conducting coreference resolution at the tectogrammatical layer.", "Finally, we compare these figures with the final scores presented in Section 5 to see a penalty for modeling coreference.", "The experiment was undergone on the DevAdd dataset (see Section 5), annotated with coreference in German, Russian and English.", "The English part was used to evaluate after the first stage whereas the German and Russian parts for the rest.", "Figure 2 illustrates how the score declines as the system proceeds over its stages (from left to right).", "The system for English evaluated after the first stage falls behind the state-of-the-art CR systems by more than 10 points.", "This can be attributed to a 33-times smaller test set as well as to gentle differences in annotation guidelines.", "Cross-lingual projection seems to be the bottleneck of the proposed approach.", "The performance drops by almost 10 points in Russian, even more in German.", "This could be partially rectified by using better alignment techniques.", "The loss incurred by operating at the tectogrammatical layer is larger for Russian.", "It can be attributed to the parsing issues observed on Russian (see Section 3.3).", "On the other hand, modeling projected coreference by machine learning harms a lot more for German.", "The models are fit using almost the same feature sets for both languages.", "Therefore, if the drop is not a consequence of the only difference in features, i.e. word embeddings for German set, it probably results from a different extent of expressive power of the feature set for the two languages.", "Overall, while our projection-based resolver for Russian is able to preserve 66% of the quality achieved by the English resolver, it is only 46% for German.", "We introduced a system for coreference resolution via projection for German and Russian.", "The system does not exploit any manually annotated data in these languages.", "Instead, it projects the automatic annotation of coreference from English to these languages through a parallel corpus.", "The resolution system operates on the level of deep syntax and takes advantage of specialized models for individual mention types.", "It seems to be more suitable for Russian as it is able to achieve 66% of the English resolver\u2019s quality, while it is less than 50% in German, both measured by CoNLL score.", "We submitted the system to the closed track of the CORBON 2017 Shared task.", "veloped and/or stored and/or distributed by the LINDAT/CLARIN project No.", "LM2015071 of the Ministry of Education, Youth and Sports of the Czech Republic."]}, {"summaries": ["We used the C4.5, random forest, and logistic regression algorithms from the Weka Toolkit and LibLinear to train the models.", "We generate a weakly labelled training set using parallel corpora, English-Swedish and English-German, where we solve the coreference for English using CoreNLP and transfer it to Swedish and German using word alignments."], "paperId": "W17-0206", "paperName": "2012-2", "document": ["Coreference resolution is the identification of phrases that refer to the same entity in a text.", "Current techniques to solve coreferences use machine-learning algorithms, which require large annotated data sets.", "Such annotated resources are not available for most languages today.", "In this paper, we describe a method for solving coreferences for Swedish and German using distant supervision that does not use manually annotated texts.", "We generate a weakly labelled training set using parallel corpora, English-Swedish and English-German, where we solve the coreference for English using CoreNLP and transfer it to Swedish and German using word alignments.", "To carry this out, we identify mentions from dependency graphs in both target languages using hand-written rules.", "Finally, we evaluate the end-to-end results using the evaluation script from the CoNLL 2012 shared task for which we obtain a score of 34.98 for Swedish and 13.16 for German and, respectively, 46.73 and 36.98 using gold mentions.", "Coreference resolution is the process of determining whether two expressions refer to the same entity and linking them in a body of text.", "The referring words and phrases are generally called mentions.", "Coreference resolution is instrumental in many language processing applications such as information extraction, the construction of knowledge graphs, text summarizing, question answering, etc.", "As most current high-performance coreference solvers use machine-learning techniques and supervised training ( CITE-p-33-1-5 ), building solvers requires large amounts of texts, hand-annotated with coreference chains.", "Unfortunately, such corpora are expensive to produce and are far from being available for all the languages, including the Nordic languages.", "Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 46\u201355, Gothenburg, Sweden, 23-24 May 2017.", "2017c Linko\u0308ping University Electronic Press", "In the case of Swedish, there seems to be only one available corpus annotated with coreferences: SUC-Core ( CITE-p-33-3-7 ), which consists of 20,000 words and 2,758 coreferring mentions.", "In comparison, the CoNLL 2012 shared task ( CITE-p-33-3-12 ) uses a training set of more than a million word and 155,560 coreferring mentions for the English language alone.", "Although models trained on large corpora do not automatically result in better solver accuracies, the two orders of magnitude difference between the English CoNLL 2012 corpus and SUC- Core has certainly consequences on the model quality for English.", " CITE-p-33-3-12  posited that larger and more consistent corpora as well as a standardized evaluation scenario would be a way to improve the results in coreference resolution.", "The same should apply to Swedish.", "Unfortunately, annotating 1,000,000 words by hand requires seems to be out of reach for this language for now.", "In this paper, we describe a distant supervision technique to train a coreference solver for Swedish and other languages lacking large annotated corpora.", "Instead of using SUC-Core to train a model, we used it for evaluation.", "Distant supervision is a form of supervised learning, though the term is sometimes used interchangeably with weak supervision and self training depending on the source ( CITE-p-33-3-6 ,  CITE-p-33-5-1 ).", "The primary difference between distant supervision and supervised learning lies in the annotation procedure of the training data; supervised learning uses labelled data, often obtained through a manual annotation, whereas in the case of distant supervision, the annotation is automatically generated from another source than the training data itself.", "Training data can be generated using various methods, such as simple heuristics or from the output of another model.", "Distant supervision will often yield models that perform less well than models using other forms of supervised learning ( CITE-p-33-5-1 ).", "The advantage of distant supervision is that the training set does not need an initial annotation.", "Distant supervision covers a wide range of methods.", "In this paper, we used an annotation projection, where the output of a coreference resolver is transferred across a parallel corpus, from English to Swedish and English to German, and used as input for training a solver in the target language ( CITE-p-33-3-5 ,  CITE-p-33-1-6 ).", "Parallel corpora have been used to transfer syntactic annotation.", " CITE-p-33-3-0  is an example of this.", "In the case of coreference, Rahman and  CITE-p-33-3-13  used statistical machine translation to align words and sentences and transfer annotated data and other entities from one language to another.", "They collected a large corpus of text in Spanish and Italian, translating each sentence using machine translation, applying a coreference solver on the generated text, and aligning the sentences using unsupervised machine translation methods.", " CITE-p-33-3-5  developed a coreference solver for Spanish and Portuguese using distant supervision, where he transferred entity mentions from English to a target language using machine-learning techniques.", "In this paper, we describe a new projection method, where we use a parallel corpus similarly to  CITE-p-33-3-0  and, where we follow the methods and metrics described by Rahman and  CITE-p-33-3-13 .", "We also reused the maximum span heuristic in  CITE-p-33-3-5  and the pruning of documents according to the ratio between correct and incorrect entity alignments.", "Our goal was to create a coreference solver for Swedish and German with no labelled data to train the model.", "Swedish has no corpora of sufficient size to train a general coreference solver, whereas German has a large labelled corpus in the form of Tu\u0308ba D/Z ( CITE-p-33-1-11 ).", "Although we could have trained a solver from the Tu\u0308ba D/Z dataset, we applied the same projection methods to German to determine if our method would generalize beyond Swedish.", "We generated weakly labelled data using a parallel corpus consisting of sentence-aligned text with a sentence mapping from English to Swedish and English to German.", "We annotated the English text using a coreference solver for English and we transferred the coreference chains to the target language by word alignment.", "We then used the transferred coreference chains to train coreference solvers for the target languages.", "We used three language-dependent processing pipelines:", "\u2022 Weal., applied2014) toStanford\u2019sannotate CoreNLPthe English(Manningpart.", "Weet used the parts of speech, dependency graphs, and coreference chains;", "\u2022 Mateman; Tools ( CITE-p-33-1-4 ) for Ger-", "\u2022 For2013Swedish) for the ,partswe ofusedspeechStaggerand MaltParser(O\u0308stling, for the dependencies ( CITE-p-33-3-8 ).", "As annotation and evaluation framework, we followed the CoNLL 2011 and 2012 shared tasks ( CITE-p-33-3-11 ,  CITE-p-33-3-12 ).", "These tasks evaluated coreference resolution systems for three languages: English, Arabic, and Chinese.", "To score the systems, they defined a set of metrics as well as a script that serves as standard in the field.", "We carried out the evaluation for both Swedish and German with this CoNLL script.", "For Swedish, we used SUC-Core ( CITE-p-33-3-7 ) as a test set, while for German, we used the Tu\u0308ba-D/Z corpus in the same manner as with SUC-Core (Henrich and Hinrichs, 2013; Henrich and Hinrichs, 2014).", "As parallel corpora, we used the Europarl corpus ( CITE-p-33-3-1 ), consisting of protocols and articles from the EU parliament gathered from 1996 in 21 language pairs.", "Europarl is a large sentence-aligned unannotated corpus consisting of both text documents and web data in the XML format.", "Each language pair has alignment files to map the respective sentences in the different languages.", "We only used the text documents in this study and we removed unaligned sentences.", " CITE-p-33-3-1  evaluated the Europarl corpus using the BLEU metric ( CITE-p-33-3-10 ).", "High BLEU scores are preferable as they often result in better word alignments ( CITE-p-33-5-2 ).", "The BLEU values for Europarl ranged from 10.3 to 40.2, with the English-to-Swedish at 24.8 and English-to-German at 17.6, where 0 means no alignment and 100 means a perfect alignment.", "Additionally,  CITE-p-33-1-0  notes that the English-Swedish alignment of Europarl contains a high share of structurally complex relations, which makes word alignment more difficult.", "To carry out the transfer of entity mentions, we aligned the sentences and the words of the parallel corpora, where English was the source language and Swedish and German, the target languages.", "Europarl aligns the documents and sentences using the Gale and Church algorithm.", "This introduces additional errors when aligning the words.", "Instead, we used the precomputed word alignments from the open parallel corpus, OPUS, where improper word alignments are mitigated ( CITE-p-33-3-2 ,  CITE-p-33-3-18 ).", "The word alignments in OPUS use the phrase-based grow-diag- final-and heuristic, which gave better results.", "Additionally, many of the challenges in aligning English to Swedish described by  CITE-p-33-1-0  appeared to be mitigated.", "From the word alignment, we carried out the mention transfer.", "We used a variation of the maximum span heuristic.", "Bilingual word alignment is complicated even under ideal circumstances as modeling errors, language differences, and slight differences in meaning may all affect the word alignment negatively.", "Figure 1 shows two examples of good and bad projections from  CITE-p-33-5-2 .", "The figures describe two projection scenarios with varying levels of complexity from a source language on the top of the figures to a target language at the bottom.", "The solid lines correspond to word alignments while the dotted lines define the boundaries of their maximum span heuristic.", " CITE-p-33-5-2  argue that even though individual word alignments are incorrect, a group of words corresponding to a noun phrase in the source language tends to be aligned with another group in the target language.", "The largest span of aligned words from a noun phrase in the target language usually corresponds to the original noun phrase in the source language.", "Following  CITE-p-33-5-2 , the maximal span heuristic is to discard any word alignment not mapped to the largest continuous span of the target language and discard overlapping alignments, where one mention is not bounded by the other mentions for each mention.", "The heuristic is nontrivial to evaluate and we primarily selected it for its simplicity, as well as its efficiency with coreference solvers for Spanish and Portuguese using distant supervision ( CITE-p-33-3-5 ).", "The maximum span heuristic uses no syntactic knowledge other than tokenization for the target language.", "We implemented a variation of the maximum span heuristic which utilizes syntactic knowledge of the target language.", "We selected the largest mention bounded by each maximum span instead of the maximum span itself.", "As result, the generated corpus would only consist of valid mentions rather than brackets of text without any relation to a mention.", "This has the additional benefit of simplifying overlapping spans as a mention has a unique head and the problem of overlapping is replaced with pruning mentions with identical bounds.", "We removed some documents in the corpus from the generated data set according to two metrics: The document length and alignment agreement as in  CITE-p-33-3-5 .", "The goal was to create a training set with comparable size to the CoNLL task, i.e. a million words or more.", "To this effect, we aligned all the documents using the maximum span variant and we measured the alignment accuracy defined as the number of accepted alignments divided by the sum of all alignments.", "We removed all the documents with lower than average alignment accuracy.", "Additionally, larger documents were removed until we could generate a total training set consisting of approximately a million words in total.", "There are multiple metrics to evaluate coreference resolution.", "We used the CoNLL 2012 score as it consists of a single value ( CITE-p-33-3-12 ).", "This score is the mean of three other metrics: MUC6 ( CITE-p-33-3-19 ),  MATH-w-17-1-0-51  (Bagga and Baldwin, 1998), and CEAF E ( CITE-p-33-3-3 ).", "We also report the values we obtained with CEAF M , and BLANC ( CITE-p-33-3-14 ).", "Swedish: SUC-Core.", "The SUC-Core corpus ( CITE-p-33-3-7 ) consists of 20,000 words and tokens in 10 documents with 2,758 coreferring mentions.", "The corpus is a subset of the SUC 2.0 corpus, annotated with noun phrase coreferential links (Gustafson-Capkova\u0301 and Hartmann, 2006).", "The corpus is much too small to train a coreference solver, but it is more than sufficient to evaluate solvers trained on some different source material.", "As a preparatory step to evaluate coreference resolution in Swedish, the information from SUC-Core was merged with SUC 2.0 and SUC 3.0 to have a CoNLL 2012 compatible file format.", "Additionally, we removed the singletons from the merged data files.", "German: Tu\u0308ba D/Z.", "The Tu\u0308ba D/Z corpus (Henrich and Hinrichs, 2013; Henrich and Hinrichs, 2014) consists of 1,787,801 words and tokens organized in 3,644 files annotated with both part of speech and dependency graph information.", "Although the corpus would be sufficient in size to train a coreference solver, we only used it for evaluation in this work.", "As with SUC-Core, we removed all the singletons.", "Due to time and memory constraints, we only used a subset of the Tu\u0308ba D/Z corpus for evaluation.", "Similarly to the CoNLL 2011 and 2012 shared tasks, we evaluated our system using gold and predicted mention boundaries.", "When given the gold mentions, the solver knows the boundaries of all nonsingleton mentions in the test set, while with predicted mention boundaries, the solver has no prior knowledge about the test set.", "We also followed the shared tasks in only using machine-annotated parses as input.", "The rationale for using gold mention boundaries is that they correspond to the use of an ideal method for mention identification, where the results are an upper bound for the solver as it does not consider singleton mentions ( CITE-p-33-3-11 ).", "For Swedish, we restricted the training set to the shortest documents containing at least one coreference chain.", "After selection and pruning, this set consisted of 4,366,897 words and 183,207 sentences in 1,717 documents.", "For German, we extracted a training set consisting of randomly selected documents containing at least one coreference chain.", "After selection and pruning, the set consisted of 9,028,208 words and 342,852 sentences in 1,717 documents.", "Swedish.", "The mentions in SUC-Core correspond to noun phrases.", "We identified them automatically from the dependency graphs produced by Maltparser using a set of rules based on the mention headwords.", "Table 1 shows these rules that consist of a part of speech and an additional constraint.", "When a rule matches the part of speech of a word, we create the mention from its yield.", "As SUC-Core does not explicitly define the mention bracketing rules, we had to further analyze this corpus to discern basic patterns and adjust the rules to better map the mention boundaries (Table 2).", "German.", "The identification of noun phrases in German proved more complicated than in Swedish, especially due to the split antecedents linked by a coordinating conjunction.", "Consider the phrase Anna and Paul.", "Anna, Paul, as well as the whole phrase are mentions of entities.", "In Swedish, the corresponding phrase would be Anna och Paul with the conjunction och as the head word.", "The annotation scheme used for the TIGER corpus does not have the conjunction as head for coordinated noun phrases ( CITE-p-33-1-2 ).", "In Swedish, the rule for identifying the same kind of split antecedents only needs to check whether a conjunction has children that were noun phrases, whereas in German the same rule required more analysis.", "Table 3 shows the rules for the identification of noun phrases in German, and Table 4, the post-processing rules.", "To solve coreference, we used a variation of the closest antecedent approach described in  CITE-p-33-3-16 .", "This approach models chains as a projected graph with mentions as vertices, where every mention has at most one antecedent and one anaphora.", "The modeling assumptions relaxes the complex relationship between coreferring mentions by only considering the relationship between a mention and its closest antecedent.", "The problem is framed as a binary classification problem, where the system only needs to decide whether a mention and its closest antecedent corefer ( CITE-p-33-3-16 ).", "When generating a training set, the negative examples are more frequent than the positive ones, which may skew the model.", "We limited the ratio at somewhere between 4 and 5 % and randomizing which negative samples become part of the final training set.", "We used the C4.5, random forest, and logistic regression algorithms from the Weka Toolkit and LibLinear to train the models ( CITE-p-33-5-0 ,  CITE-p-33-1-9 ,  CITE-p-33-1-7 ).", "Swedish.", "As features, we used a subset Stam- borg et al. (2012) and  CITE-p-33-3-16 .", "Table 5 shows the complete list.", "German.", "The feature set for German is described in Table 5.", "The primary difference between German and Swedish is the addition of gender classified names.", "We used the lists of names and job titles from IMS Hotcoref DE ( CITE-p-33-3-15 ) to train the German model.", "The morphological information from both CoreNLP and Mate Tools appeared to be limited when compared with Swedish, which is reflected in the feature set.", "Swedish.", "The Swedish EuroParl corpus consists of 8,445 documents.", "From these documents, we selected a subset consisting of 3,445 documents based on the size, where we preferred the smaller documents.", "The selected documents contained in total 1,189,557 mentions that were successfully transferred and 541,608 rejected mentions.", "We removed the documents with less than 70% successfully transferred mentions, which yielded a final tally of 515,777 successfully transferred mentions and 198,675 rejected mentions in 1,717 documents.", "German.", "The German EuroParl corpus consists of 8,446 documents.", "From these documents, we randomly selected a subset consisting of 2,568 documents.", "The selected documents contained in total 992,734 successfully transferred and 503,690 rejected mentions.", "We removed the documents with less than 60% successfully transferred mentions, which yielded a final tally of 975,539 successfully transferred mentions and 491,009 rejected mentions in 964 documents.", "Swedish.", "Using the rules described in Table 1, we identified 91.35% of the mentions in SUC- Core.", "We could improve the results to 95.82% with the additional post processing rules described in Table 2.", "German.", "Using the rules described in Table 3, we identified 65.90% of the mentions in Tu\u0308ba D/Z.", "With the additional post processing rules described in Table 4, we reached a percentage of 82.08%.", "Table 6 shows the end-to-end results when using predicted mentions and Table 7 shows the results with the same pipeline with gold mentions.", "These latter results correspond to the upper bound figures we could obtain with this technique with a same feature set.", "In this paper, we have described end-to-end coreference solvers for Swedish and German that used no annotated data.", "We used feature sets limited to simple linguistic features easily extracted from the Swedish treebank and the German Tiger corpus, respectively.", "A large subset of the feature set of  CITE-p-33-3-17  would very likely improve the results in this work.", "The results in Tables 6 and 7 show that even though the dependency grammar based approach for identifying mentions yields a decent performance compared with CoNLL 2011 and 2012, a better identification and pruning procedure would probably significantly improve the results.", "This is manifest in German, where using the gold mentions results in a considerable increase of the scores: Table 7 shows a difference of more than 23 points compared with those in Table 6.", "This demonstrates that the large difference in scores between Swedish and German has its source in the methods used for mention identification rather than in the different feature sets or the correctness of the training set.", "This can be explained by the difficulty to predict mentions for German, possibly because of the differences in the dependency grammar format, as relatively few mentions were identified using their head elements.", "The final results also show that classifiers based on J48 and random forests produced better scores than logistic regression.", "Coreference resolution using weak labelled training data from distant supervision enabled us to create coreference solvers for Swedish and German, even though the mention alignment in the parallel corpora was far from perfect.", "It is difficult to compare results we obtained in this article with those presented in CoNLL, as the languages and test sets are different.", "Despite this, we observe that when using gold mention boundaries, we reach a MELA CoNLL score for Swedish that is comparable with results obtained for Arabic in the CoNLL-2012 shared task using the similar preconditions.", "We believe this shows the method we proposed is viable.", "Our results are, however, lower than those obtained for English and Chinese in the same task and could probably be improved with a better mention detection."]}, {"summaries": ["Eye movement data driven mention pair pruning, as discussed above, is experimented across different classifiers, viz., Support Vector Machine (SVM), Naive Bayes, and Multi-layered FeedForward Neural Network (Neural Net).", "This paper aims at utilizing cognitive information obtained from the eye movements behavior of annotators for automatic coreference resolution."], "paperId": "W16-1904", "paperName": "2012-3", "document": ["This paper aims at utilizing cognitive information obtained from the eye movements behavior of annotators for automatic coreference resolution.", "We first record eye-movement behavior of multiple annotators resolving coreferences in 22 documents selected from MUC dataset.", "By inspecting the gaze-regression profiles of our participants, we observe how regressive saccades account for selection of potential antecedents for a certain anaphoric mention.", "Based on this observation, we then propose a heuristic to utilize gaze data to prune mention pairs in mention-pair model, a popular paradigm for automatic coreference resolution.", "Consistent improvement in accuracy across several classifiers is observed with our heuristic, demonstrating why cognitive data can be useful for a difficult task like coreference resolution.", "Coreference resolution deals with identifying the expressions in a discourse referring to the same entity.", "It is crucial to many information retrieval tasks ( CITE-p-10-1-8 ).", "One of its main objectives of is to resolve the noun phrases to the entities they refer to.", "Though there exist many rule based ( CITE-p-10-1-11 ,  CITE-p-10-3-3 ,  CITE-p-10-3-6 ) and machine learning based ( CITE-p-10-3-10 ,  CITE-p-10-3-4 ,  CITE-p-10-3-7 ) approaches to coreference resolution, they are way behind imitating the human process of coreference resolution.", "Comparing the performance of different existing systems on a standard dataset, Ontonotes, released for CoNLL-2012 shared task ( CITE-p-10-3-5 ), it is quite evident that the recent systems do not have much improvement in accuracy over the earlier systems (Bjo\u0308rkelund and Farkas, 2012; Durrett and Klein, 2013; Bjo\u0308rkelund and Kuhn, 2014; Martschat et al., 2015; Clark and Manning, 2015).", "This paper attempts to gain insight into the cognitive aspects of coreference resolution to improve mention-pair model, a well-known supervised coreference resolution paradigm.", "For this we employ eye-tracking technology that has been quite effective in the field of psycholinguistics to study language comprehension ( CITE-p-10-3-9 ), lexical ( CITE-p-10-3-8 ) and syntactic processing(von der Malsburg and Vasishth, 2011).", "Recently, eye-tracking studies have been conducted for various language processing tasks like Sentiment Analysis, Translation and Word Sense Disambiguation.", " CITE-p-10-1-10  develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking.", " CITE-p-10-3-2  measure complexity in text to be translated based on gaze input of translators which is used to label training data.", "Eye-tracking studies have also been conducted for the task of coreference resolution.", " CITE-p-10-1-6  check for whether the syntax or discourse representation has better role in pronoun interpretation.", " CITE-p-10-1-0  examine the effect of gender information and accessibility to pronoun interpretation.", " CITE-p-10-3-12  studies the fixation patterns on pronoun and associated verb phrases to explain comprehension of pronouns.", "We perform yet another eye-tracking study to understand certain facets of human process involved in coreference resolution that eventually can help automatic coreference resolution.", "Our participants are given a set of documents to perform coreference annotation and the eye movements during the exercise are recorded.", "Eye-movement patterns are characterized by two basic attributes: (1) Fixations, corresponding to a longer stay of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations.", "Moreover, a saccade is called a Regressive Sac- cade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment.", "While analyzing these attributes in our dataset, we observe a correlation between the Total Regression Count and the complexity of a mention being resolved.", "Additionally, Mention Regression Count, i.e., the count of a previous mention getting visited while resolving for an anaphoric mention, proves to be a measure of relevance of that particular mention as antecedent to the anaphoric mention.", "Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 22\u201326, Berlin, Germany, August 11, 2016.", "2016c Association for Computational Linguistics", "Following the insights, we try to enrich mention-pair model, a popular paradigm in automatic coreference resolution by performing mention pair pruning prior to classification using mention regression data.", "We prepared a set of 22 short documents, each having less than 10 sentences.", "These were selected from the MUC-6 dataset 1 .", "Discourse size is restricted in order to make the task simpler for the participants and to reduce eye movements error caused due to scrolling.", "The documents are annotated by 14 participants.", "Out of them, 12 of them are graduate/postgraduate students with science and engineering background in the age group of 20-30 years, with English as the primary language of academic instruction.", "The rest 2 are expert linguists and they belong to the age group of 47-50.", "To ensure that they possess good English proficiency, a small English comprehension test is carried out before the start of the experiment.", "Once they clear the comprehension test, they are given a set of instructions beforehand and are advised to seek clarifications before they proceed further.", "The instructions mention the nature of the task, annotation input method, and necessity of head movement minimization during the experiment.", "The task given to the participants is to read one document at a time, and assign ids to mentions that are already marked in the document.", "Each id corresponding to a certain mention has to be unique, such that all the coreferent mentions in a single coreference chain are assigned with the same id.", "During the annotation, eye movements data of the participants (in terms of fixations, saccades and pupil-size) are tracked using an SR-Research Eyelink-1000 Plus eye-tracker (monocular mode with sampling rate of 500 Hz).", "The eye-tracking device is calibrated at the start of each reading session.", "Participants are allowed to take breaks between two reading sessions, to prevent fatigue over time.", "We observe that the average annotation accuracy in terms of CoNLL-score ranges between 70.75%-86.81%.", "Annotation error, we believe, could be attributed to: (a) Lack of patience/attention while reading, (b) Issues related to text comprehension and understanding, and (c) Confusion/indecisiveness caused due to lack of context.", "The cognitive activity involved in resolving coreferences is reflected in the eye movements of the participants, especially in the movements to the previously visited words/phrases in the document, termed as regressive saccades or simply, regressions.", "Regression count refers to the number of times the participant has revisited a candidate antecedent mention while resolving a particular anaphoric mention.", "This is extracted from the eye movement events between the first gaze of the anaphoric mention under consideration and the annotation event of this mention (when participants annotate the mention with a coreferent id).", "Figure 1 shows the mention position (for a given mention id) in terms of the order of the mention in the document against count of regression going out from each mention to the previous mentions.", "The regression count for a particular mention is averaged over all the participants.", "As we see, average regression count tends to increase with increase in mention id, except for some mentions which may not have required visiting to the previous mentions for resolving them.", "The complexity of the content in MUC-6 dataset makes the spread of the regression counts dispersed.", "We also observe that, towards the end of the document, participants tend to regress more to the earlier sections because of limited working memory ( CITE-p-10-1-3 ).", "This increases the number of regressions performed from mentions appearing towards the end of the document.", "It is worth noting that intra-sentential mentions that have antecedents within the same sentence (as in \u2019Prime Minister Brian Mulroney and his cabinet have been briefed today\u2019) do not generally elicit regressions.", "We believe, intra-sentential resolutions are connected to processing of syntactic constraints in an organized manner, as explained by the binding theory ( CITE-p-10-1-4 ).", "Though the number of intra-sentential mentions in our dataset is low, it is evident from figure 1, that they do not account for many regressions.", "This above analysis on regression counts supports our hypothesis that the mentions that are regressed to more frequently have a better say in resolving an anaphoric mention.", "We experiment with a supervised system following a mention-pair model ( CITE-p-10-3-10 )- injecting the eye-movement information into it.", "Mention-pair model classifies mention pairs formed between mentions in a document as coreferent or not, followed by clustering, forming clusters of coreferent mentions.", "Eye tracking information is utilized in the process of mention pair pruning prior to mention pair classification.", "Given an anaphoric mention, the probability of each previous mention being selected as antecedent is computed as follows.", "Transitions done by a participant to potential antecedent mentions, while resolving an anaphoric mention, are first obtained from the regression profile.", "From this, we filter out the regressions to a candidate antecedent mention that happen between two events- (a) first fixation lands on the anaphoric mention and (b) the anaphoric mention gets annotated with an id.", "mention pairs mention pair pruning Eye tracking transition prob mention pair classification clustering coreferent chains", "These regression counts from all the participants are aggregated to compute the transition probability values, as follows:  MATH-p-6-7-0 ", "In equation 1,  MATH-w-6-8-0-4  gives the transition probability value for an anaphoric mention  MATH-w-6-8-0-20  to a candidate antecedent mention  MATH-w-6-8-0-28  .", "count() computes the aggregated regression count over all participants.", "Denominator part computes for all candidate antecedents( MATH-w-6-8-0-53 ) of the anaphoric mention.", "Transition probability thus computed for candidate mention pairs, are utilized prior to mention pair classification, filtering out irrelevant mention pairs.", "In the mention pair model, a mention pair MATH-w-6-8-1-34  is formed between an anaphoric mention (m ana ) and a candidate antecedent mention (m ant ).", "For an anaphoric mention, the threshold probability value is computed from the number of potential candidate antecedents.", " MATH-w-6-8-1-83  .", "Eye movement data driven mention pair pruning, as discussed above, is experimented across different classifiers, viz., Support Vector Machine (SVM), Naive Bayes, and Multilayered Feed-Forward Neural Network (Neural Net).", "We use libsvm 3 for SVM implementation and Scikit-Learn 4 for Naive Bayes implementation.", "The neural network classifier having an input layer, a hidden layer and an output layer is implemented using Keras 5 .", "For training, we consider a subset of English section of OntoNotes (v5.0) data ( CITE-p-10-3-5 ) with 1634 documents.", "Testing is done with the 22 documents taken from MUC-6 dataset.", "Since the main aspect of our work is mention pair pruning, we first check the mention pair pruning accuracy.", "We find that mention pair pruning has a precision of 87.24%.", "Pruning errors may be attributed to increased number of regressions happening to mentions towards the end of the documents (refer section 3).", "Performance of the system is evaluated using MUC, B 3 and CEAFe metrics.", "CoNLL score is computed as the average of F1s of all the mentioned metrics.", "Table 1 shows the results across different classifiers with and without mention pair pruning.", "Considering the CoNLL score, there is an improvement in performance across all classifiers.", "This improvement is contributed by the increase in precision , despite the fall in recall.", "Table 2 shows a few instances of non-coreferent antecedent-anaphora pairs which are correctly predicted as non-coreferent because of pruning.", "Among all the classifiers neural network gives better accuracy, but the effective performance gain is higher with classifiers with lesser accuracy.", "Naive Bayes giving the least accuracy, gives the best accuracy improvement of 2.04% with mention-pair pruning.", "This gives the impression that systems with lower performance, are likely to benefit from the eye movement based heuristics.", "The performance improvement of mention pair pruning is also verified with the state of the art Berkeley Coreference Resolution system ( CITE-p-10-1-7 ).", "The choice of the system was based on the code accessibility to make the modification required for mention pair pruning.", "Results of Berkeley system in table 1 shows that there is an improvement in CoNLL score , mainly contributed by the increase in precision.", "As far as we know, our work of utilizing cognitive information for the task of automatic coreference resolution is the first of it kind.", "By analyzing the eye-movement patterns of annotators, we observe a correlation between the complexity of resolving an anaphoric mention and eye-regression count associated with the preceding mentions.", "We also observe that gaze transition probability derived from regression counts associated with a mention signify the candidacy of that mention as an antecedent.", "This helps us devise a heuristic to prune irrelevant mention pair candidates in a supervised coreference resolution approach.", "Our heuristic brings noticeable improvement in accuracy with different classifiers.", "The current work can be further enriched to utilize eye-gaze information for (a) meaningful feature extraction for mention pair classification and (b) proposing efficient clustering mechanism.", "We would also like to replace our current annotation setting with a non-intrusive reading setting (say, reading text on mobile devices with camera based eye-trackers), where explicit annotations need not be required."]}], "queries": ["Learning Framework", "Markable Identification"]}, {"setId": "CoNLL-2013", "papers": [{"summaries": ["In this paper, we present a model based on Bidirectional Long Short-Term Memory(Bi-LSTM) neural networks, which treats the task as a sequence labeling problem, so as to detect Chinese grammatical errors, to identify the error types and to locate the error positions."], "paperId": "W16-4919", "paperName": "2013-1", "document": ["Grammatical Error Diagnosis for Chinese has always been a challenge for both foreign learners and NLP researchers, for the variousity of grammar and the flexibility of expression.", "In this paper, we present a model based on Bidirectional Long Short-Term Memory(Bi-LSTM) neural networks, which treats the task as a sequence labeling problem, so as to detect Chinese grammatical errors, to identify the error types and to locate the error positions.", "In the corpora of this year\u2019s shared task, there can be multiple errors in a single offset of a sentence, to address which, we simutaneously train three BiLSTM models sharing word embeddings which label Missing, Redundant and Selection errors respectively.", "We regard word ordering error as a special kind of word selection error which is longer during training phase, and then separate them by length during testing phase.", "In NLP-TEA 3 shared task for Chinese Grammatical Error Diagnosis(CGED), Our system achieved relatively high F1 for all the three levels in the traditional Chinese track and for the detection level in the Simpified Chinese track.", "As China plays a more and more important role of the world, learning Chinese as a foreign language is becoming a growing trend, which brings opportunities as well as challenges.", "Due to the variousity of grammar and the flexibility of expression, Chinese Grammatical Error Dignosis(CGED) poses a serious challenge to both foreign learners and NLP researchers.", "Unlike inflectional languages such as English which follows grammatical rules strictly(i.e. subject-verb agreement strict tenses and voices), Chinese,, as an isolated language, has no morphological changes.", "Various characters are arranged in a sentence to represent meanings as well as the tense and the voice.", "These features make it easy for beginners to make mistakes in speaking or writing.", "In NLP-TEA 3 shared task for Chinese Grammatical Error Diagnosis(CGED), four types of errors are defined: \u2019M\u2019 for missing word error, \u2019R\u2019 for redundant errors, \u2019S\u2019 for word selection error and \u2019W\u2019 for word ordering error.", "Some typical examples of the errors are shown in Table 1.", "Different from the two previous editions for the CGED shared task, each input sentence contains at least one of defined error types.", "What\u2019s more, there can be multiple errors in a single offset of a sentence, which means we can no longer treat it a simple multi-class classification problem.", "As a result of that, we cannot simply rely on some existing error detection systems but can only seek for a new solution.", "Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages 148\u2013154, Osaka, Japan, December 12 2016.", "In order to address the problem, we regard it as a sequence multi-labeling problem and split it into multiple sequence labeling problems which only label 0 or 1.", "To avoid feature engineering, for each error type except \u2019W\u2019, we trained a BiLSTM based neural network model, sharing word embeddings and POS tag embeddings.", "We treat the word ordering error as a special kind of word selection error.", "They are trained together and separated during the testing phase.", "Experiments show that together training is better than separate training.", "This paper is organized as follows: Section 2 briefly introduces some previous work in this area.", "Section 3 describes the BiLSTM neural network model we proposed for this task.", "Section 4 demonstrates the data analysis and some interesting findings.", "Section 5 shows the data analysis and the evaluation results.", "Section 6 concludes this paper and illustrates the future work.", "Grammatical error detection and correction has been studied with considerable efforts in the NLP community.", "Compared to Chinese, the language of English attracted more attention from the researchers, especially during the CoNLL2013 and 2014 shared task ( CITE-p-11-1-0 ,  CITE-p-11-1-1 ).", "However, different from English which has various language materials and annotated corpura, the grammatical error correction related resource for Chinese is far from enough.", "We are glad to see the shared tasks on CGED ( CITE-p-11-5-3 ,  CITE-p-11-5-4 ) in last two years.", "There were some previous related work for Chinese grammatical error detection or correction.", " CITE-p-11-1-2  proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to three of the types in the shared task.", "Experimental results showed syntactic features, web corpus features and perturbation features are useful for word ordering error detection ( CITE-p-11-1-3 ).", "A set of handcrafted linguistic rules with syntactic information are used to detect errors occurred in Chinese sentences ( CITE-p-11-5-0 ), which are shown to achieve good results.", " CITE-p-11-5-1  introduced a sentence level judgment system which integrated several predefined rules and N-gram based statistical features.", "Our submission was an exploration to a neural network model in CGED which didn\u2019t need any feature selection efforts.", "As a model well known for its good maintainance of both preceding and succeeding information, BiLSTM came to be the first choice.", "We regard CGED task as a word-based sequence multi-labeling problem, by labeling each word zero or more tags from  MATH-w-4-1-0-20 .", "For some reason described in Section 4, we treat word ordering error as a special kind of word selection error, as a result of which, we need to deal with only three kinds of error types during the training phase.", "As different error types are relatively independent from each other from a single word\u2019s perspective, we can train three sequence labeling models to judge whether this kind of error ocurrs in a certain position for each error type respectively.", "As shown in Figure 1, the architecture of a BiLSTM neural network model for CGED for a single error type can be characterized by the following three specialized layers: (1) Embedding layer (2) Encoding layer (3) Decoding layer.", "As the errors are judged on words instead of characters, we first segment the input sentence into individual words using the CKIP Chinese Segmentation System 1 provided by Taiwan Academia Sinica and get the simplified Part-of-Speech tag for each corresponding segmented word, which is our preprocess for the system.", "The words and POS tags are then embeded in the embedding layer.", "The most common tagging approach is the window approach.", "The window approach assumes that the tag of a word largely depends on its neighboring words.", "For each word  MATH-w-4-6-1-30  in a given input sentence  MATH-w-4-6-1-37  , the context words  MATH-w-4-6-1-47  and the context POS tags  MATH-w-4-6-1-64  are chosen to be fed into the networks, where  MATH-w-4-6-1-86  is the context window size and usually  MATH-w-4-6-1-94  or  MATH-w-4-6-1-98 .", "Here we set  MATH-w-4-6-1-105  in our experiments.", "The words and POS tags exceeding the sentence boundaries are mapped to two special symbols, \u201c<BOS>\u201d and \u201c<EOS>\u201d, representing Beginning of a Sentence and End of a Sentence respectively.", "Given a word set  MATH-w-4-6-2-4 , the embedding layer will map each word  MATH-w-4-6-2-18  into a  MATH-w-4-6-2-23  -dimensional embedding space as  MATH-w-4-6-2-29  by a lookup table  MATH-w-4-6-2-42  .", "In the same way, we can map each POS tag  MATH-w-4-6-2-64  -dimensional embedding space as  MATH-w-4-6-2-75  by a lookup table  MATH-w-4-6-2-88  , where  MATH-w-4-6-2-100  is the POS tag set whose size if  MATH-w-4-6-2-109 .", "The embeddings of the context words  MATH-w-4-6-2-119  and The embeddings of the context POS tags  MATH-w-4-6-2-139  are then concatenated into a single vector  MATH-w-4-6-2-158  , where  MATH-w-4-6-2-166 .", "The encoding layer is a BiLSTM layer followed by a full-connection layer, which can be simply expressed by the following:  MATH-p-4-7-0  (1) (2)   where  MATH-w-4-8-0-1  is the inner parameters of the BiLSTM layer and  MATH-w-4-8-0-11  is the logistc sigmoid function.", "The Long Short-Term Memory cell( CITE-p-11-5-2 ) is a special kind of the RNN cell which replaces the hidden layer updates by purpose-built memory cells.", "As a result, they can utilize long range dependencies and realize the function just like memory.", "The LSTM cell is implemented as the following:  MATH-p-4-15-0 ", "where  MATH-w-4-16-0-1  is the logistic sigmoid function, and  MATH-w-4-16-0-9 ,  MATH-w-4-16-0-11 ,  MATH-w-4-16-0-13  and  MATH-w-4-16-0-15  are the input gate, forget gate, output gate and the cell, all of which are the same size as the hidden output  MATH-w-4-16-0-41 .", "The subscripts of the weight matrix describe the meaning as the name suggests.", "For instance,  MATH-w-4-16-0-60  is the input gate weight matrix for input  MATH-w-4-16-0-70 .", "A single LSTM forward layer can only utilize the previous information, which is not enough for grammatical error detection, where sometimes the error can only be inferred from the following words.", "Therefore, a bidirectional LSTM layer is proposed ( CITE-p-11-5-2 ), which can be regarded as a simple stack of a forward LSTM layer and a backward LSTM layer.", "The output of the encoding layer is then fed into a decoding layer, which is another full-connection layer with 1 output size.", "The output layer is implemented as the following:  MATH-p-4-17-0  (4) (5)   where  MATH-w-4-18-0-1  is still the logistic sigmoid function and  MATH-w-4-18-0-9  indicates whether there is an error of this type on the word  MATH-w-4-18-0-26  or not.", "As there are many more non-errors than errors in a sentence from a word perspective, the model always tends to label 0, which means correct for the word, if without any balance.", "Thus we assigned weights for the loss function, in order to rebalance the correct and incorrect labels.", "The loss function without regularization is calculated as follows:  MATH-p-4-21-0  where  MATH-w-4-22-0-1  is the coefficient on the positive examples.", "We can decide if there are errors from  MATH-w-4-22-1-8  through the model described above.", "Then we separate the \u2019S\u2019 and \u2019W\u2019 tags according to the successive word length of the error during the testing phase.", "If the length is 1 the error is a word selection error, otherwise it is a word ordering error if the length is greater than 1.", "In the TOCFL track, the data we used for training includes training and testing data from NLP-TEA 1 ( CITE-p-11-5-3 ), training data from NLP-TEA 2 ( CITE-p-11-5-4 ) and training data from NLP-TEA, 3.", "We used the testing data from NLP-TEA 2 for validation.", "In the HSK track, despite of the training set provided by the organizers, we simplified the training data from TOCFL track as supplements.", "However, the simplified data from TOCFL track seem to be no use to the evaluation results.", "Due to the limitation of time and resource, the word embeddings and POS tag embeddings we used are all random initialized.", "Take NLP-TEA 3 TOCFL dataset as an example, as there are 1384 word ordering errors in the training set, which takes only 5.5% in all 24831 errors.", "It is difficult to train this kind of errors without rebalance or resampling.", "Thus we came up with a new method, by treating word ordering error as a special kind of word selection error.", "Suprisingly, in the training set, after word segmentation, most word selection errors are within one word and all word ordering errors are longer than one word, we can easily separate them by the successive error length.", "In the formal run of NLP-TEA 3 CGED shared task, there are 5 teams submitting 15 runs in total for the TOCFL dataset track and 8 teams submitting 21 runs in total for the HSK dataset track.", "Our system achieved relatively high F1 for all the three levels in the traditional Chinese track and for the detection level in the Simpified Chinese track.", "Since our evaluation results for HSK dataset are not good, here we only display the evaluation results compared with the average values for TOCFL dataset.", "The performance evaluations in detection level, identification level and position level are shown as follows:", "In this paper, we present a BiLSTM neural network based model to predict the possible grammatical errors for Chinese, which needs no feature engineering and provides reasonable evaluation results in the NLP-TEA 3 CGED shared task.", "Different from most previous work, we didn\u2019t use any external corpus or rule-based inductions.", "Due to the limitation of time and resource, we didn\u2019t test our system under various experiment environments.", "More neural network architectures and more features can be tried."]}, {"summaries": ["We have presented a new method for extracting edits from a parallel original and corrected sentence pair based on a linguistically enhanced token alignment and rule-based merging component."], "paperId": "C16-1079", "paperName": "2013-2", "document": ["We propose a new method of automatically extracting learner errors from parallel English as a Second Language (ESL) sentences in an effort to regularise annotation formats and reduce inconsistencies.", "Specifically, given an original and corrected sentence, our method first uses a linguistically enhanced alignment algorithm to determine the most likely mappings between tokens, and secondly employs a rule-based function to decide which alignments should be merged.", "Our method beats all previous approaches on the tested datasets, achieving state-of-the-art results for automatic error extraction.", "Within the field of Machine Translation (MT), one of the first steps of data processing is to align a source sentence with a target sentence.", "This is necessary because we want to determine which tokens and phrases in the source language map to which equivalent tokens or phrases in the target language.", "As this would be extremely time consuming to do manually, several tools, such as GIZA++ ( CITE-p-16-1-8 ), have been made available to do this automatically.", "Within the related field of Grammatical Error Correction (GEC), we similarly want to align a source sentence with a target sentence to map errors to corrections (sometimes referred to as \u2018correction detection\u2019; see example in Table 1).", "However, unlike in MT, the source and target sentences in GEC are in the same language and so a majority of tokens match.", "This means alignment is comparatively more straightforward and so it is more feasible to annotate texts manually, rather than automatically.", "In fact two of the largest publicly available GEC datasets, the First Certificate in English (FCE) corpus ( CITE-p-16-3-2 ) and the National University of Singapore Corpus of Learner English (NUCLE) ( CITE-p-16-1-2 ), were aligned and annotated manually.", "Nevertheless, automatic alignment of GEC data still has several advantages over manual alignment, not least because the latter is slow, laborious work.", "This is especially important for datasets that do not always contain explicit alignments, such as Lang-8 ( CITE-p-16-1-4 ), or GEC system output that needs to be aligned to the original uncorrected sentence.", "Another important benefit of an automatic alignment is that it tends to be more consistent than a human alignment.", "For example, within both the FCE and NUCLE, strings such as has eating are inconsistently corrected as [has \u2192 was] or [has eating \u2192 was eating] even though they fundamentally equate to the same thing.", "In fact, the latter seems less desirable given the token eating does not actually change.", "A similar case is [has eating \u2192 was eaten], which is inconsistently realised either as one edit, as above, or two edits: [has \u2192 was] and [eating \u2192 eaten].", "Ultimately, it seems desirable to regularise such edits and hence reduce ambiguity in the data.", "If all datasets are treated in the same way, this would also make them fully compatible with each other.", "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 825\u2013835, Osaka, Japan, December 11-17 2016.", "Finally, automatic alignment can also simplify the annotation of new data.", "For instance,  CITE-p-16-1-9  recently claimed that forcing annotators to annotate grammatical errors within the confines of an error scheme often led to unnatural sounding sentences and that unconstrained editing correlated more with human judgements.", "As such, if we no longer ask humans to explicitly mark edit boundaries in new data, we would need to extract this information automatically.", "This is particularly useful for English as a Second Language (ESL) teaching, where teachers could edit text freely and then let a computer delimit the edits.", "There is very little previous work on automatic alignment of sentences for GEC.", "The first attempt was made by Swanson and  CITE-p-16-3-0 , who built a system to align sentences and then classify the non-match tokens type for the purposes of ESL feedback.", "In particular, they used the well-known Levenshtein distance to align the sentences and then classified any non-matches according to the FCE error scheme ( CITE-p-16-1-7 ) using a maximum entropy classifier.", "One complication noted by Swanson and Yamangil is that edits do not necessarily consist of just a single token.", "For instance, reordering errors (e.g. [only can \u2192 can only]) or errors involving phrasal verbs (e.g. [look at \u2192 watch]) necessarily consist of more than one token on at least one side of the edit.", "The Levenshtein distance, however, only aligns individual tokens and so some alignments must be merged in order to obtain multi-token edits.", "Swanson and Yamangil hence experimented with some basic merging strategies and found that simply merging all adjacent non-match alignments most closely approximated human alignments.", "Building on this foundation, Xue and  CITE-p-16-3-1  carried out an analysis of Swanson and Yamangil\u2019s work and found that approximately 70% of all errors in their error type classifier were the result of bad alignments (merged or otherwise).", "In order to improve on the simple all-merge alignment strategy, they hence trained a binary maximum entropy classifier to determine whether edits should be merged or not.", "They tested this merging classifier on several datasets, including NUCLE and the FCE, and reported improvements of between 5-10% for both alignment and classification compared to Swanson and Yamangil.", "Despite these improvements, however, there is still a considerable margin between automatic and human edit annotation.", "In addition, both approaches require training on existing annotations, which vary across datasets and can often be inconsistent.", "Ultimately, training on different datasets leads to different results and so undermines any effort towards data standardisation.", "A high-quality alignment between an original and corrected sentence is crucial for deriving meaningful edits.", "Unfortunately, however, the most common method of aligning sentences is to use the Levenshtein distance, which only optimises in terms of insertions, deletions and substitutions.", "This means that, while optimal in terms of edit operation, the alignments do not take linguistic information into account and are hence not optimal in terms of human intuition (see Table 2 (a)).", "Human alignments, on the other hand, do make use of linguistic information, so we propose automatic alignments should do the same.", "First, however, as noted by Xue and  CITE-p-16-3-1 , another limitation of Levenshtein is that it is unable to handle word order errors.", "For example, [only can \u2192 can only] is realised as [only \u2192 \u00d8], [can \u2192 can] and [\u00d8 \u2192 only]; in other words, reorderings are treated as deletions followed by insertions of identical tokens.", "Since we also need to preserve word order errors in the data, we argue that the Damerau-Levenshtein distance is better suited for the task than standard Levenshtein because it allows for token transpositions.", "function DL_distance_extended(a, b): declare d[0.", ".", "length(a), 0.", ".", "length(b)] for i := 0 to length(a) inclusive do d[i, 0] := i for j := 0 to length(b) inclusive do d[0, j] := j for i := 1 to length(a) inclusive do for j := 1 to length(b) inclusive do if a[i] = b[j] then d[i, j] := 0 else d[i, j] := min(d[i-1, j ] + del_cost(a[i]), d[i , j-1] + ins_cost(b[j]), d[i-1, j-1] + sub_cost(a[i], b[j])) // Damerau-Levenshtein extension for multi-token transpositions k = 1 while i > 1 and j > 1 and (i - k) >= 1 and (j - k) >= 1 and d[i-k, jk] - d[i-k-1, jk-1] > 0 do if sorted(lowercase(a[i-k:i+1])) = sorted(lowercase(b[jk:j+1])) then d[i, j] := min(d[i, j], d[i-k, j-k] + trans_cost(a[i-k:i+1], b[j-k:j+1]) break k += 1 return d[length(a), length(b)]", "As the majority of word order errors in NUCLE and FCE data tend to only involve two tokens, this implies that the standard Damerau-Levenshtein distance, which is likewise only able to handle two-token transpositions, is generally sufficient for our purposes.", "Nevertheless, while it might seem acceptable to ignore the longer word order errors, this ultimately means they will be broken up into smaller and less meaningful edits which will increase the overall number of false positives and false negatives in the alignment.", "To overcome this problem, we extend the Damerau-Levenshtein distance to allow for transpositions of arbitrary length, as shown in Listing 1.", "This is achieved by traversing a diagonal back from the current cell in the cost matrix and looking for a source sequence that would match the target sequence in any order.", "The cost of a transposition of length  MATH-w-5-5-1-63  is defined as  MATH-w-5-5-1-67 , which is compatible with the original definition.", "In an effort to incorporate linguistic information into the alignment, we replaced the substitution cost in Damerau-Levenshtein with the function shown in Listing 2.", "In this function, we set the cost to 0 if the original and corrected tokens differ only in case (e.g. [the \u2192 The]), otherwise, the substitution cost is the sum of sub-costs for lemma, part of speech and character differences.", "lemma cost: 0 if tokens share the same lemma or derivationally related form (e.g. \u2018met\u2019 and \u2018meeting\u2019), otherwise 0.499.", "part-of-speech cost: 0 if tokens share the same part of speech, otherwise 0.25 if both tokens are content words (adjectives, adverbs, nouns or verbs) and 0.5 in all other cases.", "character cost: the proportion of character mismatches between 0 and 1, computed as the character-level Damerau-Levenshtein distance between the tokens divided by the length of their alignment.", "To increase the likelihood of aligning derivationally related forms, we lemmatise each token as if it were an adjective, adverb, noun and verb.", "We do this because if we only lemmatise for a single part of speech, then we might overlook certain derivationally related words.", "For example, while the lemma of the verb \u2018met\u2019 is \u2018meet\u2019, the lemma of the noun \u2018meeting\u2019 is \u2018meeting\u2019, which suggests these words are not related.", "By also lemmatising \u2018meeting\u2019 as a verb however, we find that the two tokens do share a common lemma, \u2018meet\u2019, which instead correctly suggests they are related and should align.", "Ultimately, we consider two tokens to be derivationally related if their respective sets of lemmas intersect.", "The sub-costs are also set in such a way that the overall substitution function always yields values in the [0, 2) range.", "Keeping the cost asymptotic to 2 is important to enforce a preference for substitutions over insertions and deletions (both set to 1); this is why we use a lemma cost of 0.499 instead of 0.5.", "We tried different combinations of these costs, provided they met this condition, but did not find significant differences in the results.", "By incorporating all this additional linguistic information into the cost, we improve the likelihood that tokens with a similar etymology, spelling or function will align.", "This is better than the simple surface matching used by the standard token-level Levenshtein distance and hence, we argue, results in more natural, human-like alignments (see Table 2 (b)).", "The final alignment is retrieved by collecting the operations that make up the optimal path in the cost matrix.", "Given that the cost is now dependent upon a variable function, it is often the case that there is just a single optimal alignment.", "We evaluated our improved alignment algorithm using the public FCE ( CITE-p-16-3-2 ), NUCLE corpus ( CITE-p-16-1-3 ), and CoNLL test sets ( CITE-p-16-1-5 ,  CITE-p-16-1-6 ).", "While the CoNLL data is available in a pretokenised format, the FCE data is not, and so to keep things comparable, we only worked with the untokenized CoNLL data.", "It should be noted that processing each of these datasets in a standard way is not at all straightforward.", "For example, unlike the CoNLL data, the FCE contains nested edits; e.g. [entery \u2192 entry \u2192 entrance] indicates a spelling error followed by a replacement noun error.", "Similarly, the NUCLE corpus can be quite noisy and it is not uncommon for annotators to select entire paragraphs or even essays as edits with comments such as \u201cRewrite in the 3rd person\u201d, which, in the context of edit extraction, should definitely be ignored.", "In addition to these, a more general problem concerns converting character level edit spans into token level edit spans; there is no guarantee that a human-annotated character span will map exactly to a token span, which has consequences for token-based processing and evaluation.", "Similarly, some edits change sentence boundaries, which subsequently makes aligning original sentences with corrected sentences a lot more complicated, especially when there are multiple annotators.", "Ultimately, we refer the reader to Bryant and  CITE-p-16-1-0  for more information about the challenges involved in processing these datasets and for details about how we overcame them in our implementation.", "Having preprocessed the data, we used spaCy 1 v0.101.0 to tokenize (words and sentences), part of speech (POS) tag and lemmatise each sentence.", "The basic statistics of each processed dataset are shown in Table 3.", "To establish a baseline, we simply compared a standard Levenshtein alignment against the human alignments of the CoNLL 2013, CoNLL 2014 (each annotator individually) and FCE test sets (Table 4).", "The results show that Levenshtein alone does not perform particularly well at this task and is only able to achieve F 1 scores of about 50%.", "In addition to improving alignment quality, another aim of our work is to attempt to standardise edit annotation.", "As mentioned previously, human annotations sometimes include tokens that do not change; e.g. [has eating \u2192 was eating].", "Automatic alignments will never match these human edits, however, because any token that is common to both sides will be considered a match and hence not part of an edit.", "This is undesirable, so we also minimised the gold human reference edits by recursively removing tokens that were common to both sides of the edit from the right and left hand sides.", "This also removes edits that annotators detected, but were unable to correct.", "Results showing the effect of this minimisation, as well as of comparing Levenshtein against our linguistically enhanced Damerau-Levenshtein approach, are also shown in Table 4.", "The first thing to notice about these results is that the scores for the minimised gold reference are typically substantially higher than for the unmodified gold reference.", "In the case of CoNLL 2014 (1), using the minimised gold reference even shows an improvement of almost 15% F 1 .", "While the increase in score is less pronounced for FCE-test, at just under 5% F 1 , this nevertheless demonstrates the high degree of variability in the way GEC data is annotated and that it is highly desirable to standardise annotations.", "Comparing Levenshtein against our own approach, we again see a significant improvement, with scores greater than 70% F 1 on some datasets.", "This is significant, because these results are in spite of the fact that we fail to match all multi-token edits given that we do not yet merge any alignments.", "The output of the automatic alignment is a list of individual token-level operations that map the original sentence to the corrected sentence in terms of insertions, deletions, substitutions and transpositions.", "An example of this is shown in Table 5.", "Each of these operations involves one token at most in either sentence, except in the case of transpositions, which involve 2 or more tokens in both sentences.", "In most cases, these individual operations already represent a complete error: [propaganda \u2192 publicity] is a word choice error, [benefits only \u2192 only benefits] is a word order error and [companys \u2192 companies] is a noun inflection error.", "Other errors, however, may involve more than one single operation.", "For example, the correction [wide spread \u2192 widespread] in Table 5 involves two individual operations: a substitution ([wide \u2192 widespread]) and a deletion ([spread \u2192 \u00d8]).", "The statistics in Table 6 show that most errors in NUCLE and FCE-train involve only a single token on either side of an edit (i.e. 0:1, 1:0 or 1:1), and so a simple all-split strategy that merges nothing is likely to cover most of these edits.", "In fact this explains why the results in Table 4 are so high; just using Damerau-Levenshtein is equivalent to the all-split setting.", "Nevertheless, multi-token edits still form an important class of learner errors and so we should attempt to handle them.", "In order to improve performance and capture multi-token edits, we hence implemented a recursive rule-based merging function.", "First, we analysed the relationship between human annotations and how they mapped to alignment operations in NUCLE and FCE-train.", "For example, we found that the most common multi-token errors involved phrasal verbs, such as [look at \u2192 watch]; possessive nouns, such as [friends \u2192 friend \u2019s]; or orthographic changes, such as [wide spread \u2192 widespread].", "Second, we wrote rules to merge or separate alignments based on these observed patterns.", "The complete list of rules and their priority is as follows:", "1.", "Any match operation (M) breaks a sequence into subsequences that are processed individually, e.g. MDDSMMTMSI is split into DDS, T and SI.", "2.", "Any operation that involves punctuation and is followed by a token that changes case is merged, e.g. [, \u2192 .]", "+ [we \u2192 We] becomes [, we \u2192 .", "3.", "Transpositions are returned as individual edits, e.g. [only can \u2192 can only].", "4.", "Any operation that involves a possessive suffix is merged with any previous operations, e.g. [freinds \u2192 friend] + [\u00d8 \u2192 \u2019s] becomes [freinds \u2192 friend \u2019s].", "5.", "Operations that add or remove whitespace between tokens are merged, even if they have unmatched apostrophes, e.g. [sub \u2192 subway] + [way \u2192 \u00d8] = [sub way \u2192 subway].", "6.", "Substitutions between very similar tokens ( MATH-w-10-2-5-8  character matches) are returned as individual edits, e.g. [writting \u2192 writing], unless they have the same POS as the previous token, e.g. [eated \u2192 have eaten]; the verb phrase would be split without this exception.", "7.", "Substitutions preceded by another substitution are returned as individual edits.", "8.", "Any combination of operations that involves at least one content word is merged, e.g. [On \u2192 In] + [the \u2192 \u00d8] + [other \u2192 \u00d8] + [hand \u2192 addition] = [On the other hand \u2192 In addition].", "9.", "Consecutive operations that involve tokens with the same part of speech are merged, e.g. [because \u2192 \u00d8] + [of \u2192 for] = [because of \u2192 for].", "10.", "Any determiner at the end of a sequence is returned as an individual edit.", "Each sequence of alignment operations between two sentences (e.g. MSDSTDSSM) is processed recursively using the above rules in a top-down fashion.", "Rules are applied in order, with priority relative to their position in the list.", "Every time an edit is returned by one of the rules, we process the remaining subsequences individually until they are exhausted or no more rules can be applied (see Figure 1).", "It should be noted that rules that iteratively grow the merge range of the alignment (e.g. #8) can be overridden by others with higher priority (e.g. #4), causing the remaining operations in the truncated subsequence to be reprocessed from scratch.", "We evaluated our rule-based merging method on the CoNLL 2013, CoNLL 2014 (each annotator individually) and FCE test sets, and contrasted it against the following merging strategies: all-split: All consecutive non-matches are split, e.g. DDSI \u2192 D, D, S, I.", "All of these methods were applied to the output of our enhanced Damerau-Levenshtein alignment described in the previous section.", "In addition to evaluating edit extraction against a minimised reference, we also evaluate error type classification on the merged output to replicate results for an end-to-end classification system.", "For this purpose, we retrained Xue and Hwa\u2019s publicly available implementation of their MaxEnt error type classifier 2 separately on NUCLE and FCE-train.", "This classifier was based on work by Swanson and Yamangil and is used in all classification experiments.", "Results for both tasks are reported in Table 7 and reveal that our method achieves the best performance on all tasks and datasets.", "For edit extraction (i.e. merging), improvements in F 1 range between 4% and 12% over the second-best method (all-merge).", "We also observe that while all-split tends to have the highest number of TPs and lowest number of FNs, it also has the highest number of FPs, which shows how ignoring multi-token edits affects performance.", "In contrast, the all-merge strategy has the lowest number of FPs, but at the cost of also having the lowest number of TPs and highest number of FNs.", "Table 7 also reports performance on error classification, given edit extraction, revealing how each merging strategy affects automatic error type prediction.", "Results are consistent with edit extraction, although they are (expectedly) lower by an average 21.1% F 1 .", "Improvements in F 1 between our method and the second-best range between 3.9% and 9.0%.", "While CoNLL 2014 (1) achieved the best result for edit extraction, FCE-test achieved the best result for error classification.", "This might be because the two datasets are annotated according to different error classification frameworks and the FCE annotation is more consistent than NUCLE annotations.", "It is worth stating that many of our reported results are actually an underestimate of true performance.", "This is because, despite gold reference minimisation, there is a high degree of variability in the way humans annotate this sort of data.", "For instance, as shown by Bryant and  CITE-p-16-1-1 , human annotators often have very different perceptions of grammaticality and it is linguistically plausible that, for example, [has eaten \u2192 was eating] is annotated either as one edit (as above) or two edits ([has \u2192 was] + [eaten \u2192 eating]) by different, or even the same, annotators.", "This means the all-split merging strategy will never match the former while the all-merge merging strategy will never match the latter, even though the annotations fundamentally equate to the same thing.", "In contrast, we consider merge consistency a strength of our rule-based approach.", "Even if our alignment does not agree with the gold standard in some cases, at least the decision to merge or split is consistent across all similar cases.", "Our approach could hence be used to standardise ambiguous annotations where splits or merges are equally plausible.", "Another strength of a rule-based approach is that it is easier to diagnose which rules are responsible for producing a given output sequence.", "This is in contrast with machine learning techniques, which additionally require feature engineering and retraining, where it is much more difficult to determine why certain edits were merged in a certain way.", "Nevertheless, considering only about 30% of all edits (at most) in any dataset require merging anyway, a rule-based approach seemed a more cost-effective solution.", "We also investigated how our approach compared against previous approaches in terms of single-token edits and multi-token edits (Table 8).", "To produce comparable results for Xue and Hwa (X&H), we retrained their publicly available implementation of their MaxEnt merging classifier separately on NUCLE and FCE-train.", "In general, while our method tends to score slightly lower precision than the others in the single-token setting, it makes up for this with a much higher recall.", "In contrast, our method achieves a higher precision in the multi-token setting, but at the cost of a lower recall.", "Ultimately, however, our method increases overall performance in almost all cases, the exception being multi-token edits in CoNLL 2014 (0), which is known to be an inconsistent dataset.", "These results hence confirm that our rule-based merging strategy is superior to previous approaches.", "In addition to a quantitative analysis, we also carried out an informal qualitative analysis of the errors made by our system.", "One source of errors involves tokens that are affected by more than one mistake; e.g. [wide spraed \u2192 widespread].", "While our system includes a rule to merge adjacent alignments where the only difference is white space, this rule does not activate in the above case since one of the tokens also contains a misspelling.", "This consequently means the alignments are not merged and do not match the gold standard; such cases are difficult to handle.", "Another issue is that reference minimisation is unable to deal with edits where identical tokens occur in the middle of an edit; e.g. [can easily been \u2192 could easily be].", "As an automatic alignment will always consider easily a matched token, the remaining non-matches will become isolated and hence never merged.", "In this case, however, we would argue that our automatic alignment is more informative than the human alignment which needlessly includes a redundant token in the edit.", "Finally, we provide a comparison between our method and the previous approaches by Swanson and Yamangil (S&Y) and Xue and Hwa (X&H) (Table 9).", "Our method achieves state-of-the-art performance on all tasks and datasets, with an average improvement over X&H of 6.0% F 1 for edit extraction and 4.1% F 1 when adding error classification.", "We have presented a new method for extracting edits from a parallel original and corrected sentence pair based on a linguistically enhanced token alignment and rule-based merging component.", "Results on a number of GEC test sets show that our method outperforms all previous work on edit extraction and can also boost error classification performance.", "Since we only use a few hand-coded rules, we do away with the complexity of machine learning solutions and are hence also able to annotate data much more consistently.", "This is particularly useful for standardising GEC datasets, which are often annotated using different guidelines."]}, {"summaries": ["In this paper, we use supervised learning methods to solve the error detection and identification sub tasks."], "paperId": "W15-4415", "paperName": "2013-3", "document": ["Automatic grammatical error detection for Chinese has been a big challenge for NLP researchers for a long time, mostly due to the flexible and irregular ways in the expressing of this language.", "Strictly speaking, there is no evidence of a series of formal and strict grammar rules for Chinese, especially for the spoken Chinese, making it hard for foreigners to master this language.", "The CFL shared task provides a platform for the researchers to develop automatic engines to detect grammatical errors based on a number of manually annotated Chinese spoken sentences.", "This paper introduces HITSZ\u2019s system for this year\u2019s Chinese grammatical error diagnosis (CGED) task.", "Similar to the last year\u2019s task, we put our emphasis mostly on the error detection level and error type identification level but did little for the position level.", "For all our models, we simply use supervised machine learning methods constrained to the given training corpus, with neither any heuristic rules nor any other referenced materials (except for the last years\u2019 data).", "Among the three runs of results we submitted, the one using the ensemble classifier Random Feature Subspace (HITSZ_Run1) gained the best performance, with an optimal F1 of 0.6648 for the detection level and 0.2675 for the identification level.", "Automatic grammatical error detection for Chinese has been a big challenge for NLP researchers for a long time, mostly due to the flexible and irregular ways in the expressing of this language.", "Different from English which follows grammatical rules strictly (i.e. subject-verb agreement, or strict tenses and modals), the Chinese language has no verb tenses or numbers and endures heavily for the incompleteness of grammatical elements in a sentence (i.e. the zero subject or verb or object).", "Some examples are shown below in Table 1.", "grammatical usage in Chinese.", "In the above table, the first sentence contains no verb elements in the Chinese version.", "In the Chinese language, the adjectives will not co-occur with copulas in many cases.", "So if we add a be ( \u662f ) into the sentence ( \u56db\u6708/\u662f/\u6700/\u71b1 ), it will be grammatically incorrect.", "In the second sentence, the conjunction \u5c31 has nothing to do with the meaning of the whole sentence, but it is a necessary grammatical component when collocate with the word \u4e00 to express the meaning of as soon as.", "The adverb \u5f88 is an essential element for the third sentence which corresponds to the word very in the English version.", "However, we can simply remove very but cannot remove \u5f88 due to some implicit grammatical rules.", "Overall, the expression of the Chinese language is flexible and the grammar of Chinese is complicated and sometimes hard to summarize, so that it is very difficult for foreign language learners to learn Chinese as the second language.", "The CFL14 and 15 shared tasks provide a platform for learners and researchers to observe various cases of grammatical errors and think deeper about the intrinsic of these errors.", "The goal of the shared task is to develop computer-assisted tools to help detect four types of grammatical errors in the written Chinese.", "The error types include Missing, Redundant, Disorder and Selection.", "And in last years shared task, several groups submitted their report, employing different supervised learning methods in which some groups obtained good results in detection and classification ( CITE-p-16-1-10 ).", "Similar to the last year\u2019s task, we put our emphasis mostly on the error detection level and error type identification level but did little for the position level although this year\u2019s task includes the evaluation on this level.", "Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 99\u2013104, Beijing, China, July 31, 2015.", "2015c Association for Computational Linguistics and Asian Federation of Natural Language Processing", "In this paper, we use supervised learning methods to solve the error detection and identification sub tasks.", "Different from most of previous work, we didn\u2019t use any external language materials except for the dataset for the year 2014\u2019s shared task.", "What we adopt include feature extraction, data construction and ensemble learning.", "We also report some of our observations towards the errors and summarize some conceivable rules, which might be useful for future developers.", "At last, we analyze the limitation of our work and propose several directions for improvement.", "The following of this paper is organized as: Section 2 briefly introduces the literature in this community.", "Section 3 shows some observations towards the data provided.", "Section 4 introduces the feature extraction and learning methods we used for the shared task.", "Section 5 includes experiments and result analysis.", "In the community of grammatical error correction, more work focused on the language of English such as those researches during the CoNLL2013 and 2014 shared tasks ( CITE-p-16-1-7 ,  CITE-p-16-1-4 ).", "A number of English language materials and annotated corpus can be used such that the research on this language went deeper.", "However, the resource for Chinese is far from enough, and very few previous works are related to Chinese grammatical error correction.", "Typical ones are the CFL 2014 shared task ( CITE-p-16-1-10 ) ant the task held in this year.", "Following, we briefly introduce some previous work related to Chinese grammatical error diagnosis.", "Wu et al. proposed two types of language models to detect the error types of word order, omission and redundant, corresponding to three of the types in the shared task.", " CITE-p-16-1-0  proposed a probabilistic first-order inductive learning algorithm for error classification and outperformed some basic classifiers.", " CITE-p-16-1-4  introduced a sentence level judgment system which integrated several predefined rules and N-gram based statistical features.", " CITE-p-16-1-2  shown several methods including CRF and SVM, together with frequency learning from a large N-gram corpus, to detect and correct word ordering errors.", "In the last year\u2019s shared task, there are also some novel ideas and results for the error diagnosis.", " CITE-p-16-1-1 \u2019s work included manually constructed rules and rules that automatically generated, the latter of which are something like frequent patterns from the training corpus.", " CITE-p-16-1-12 \u2019s employed a parallel corpus from the web, which is a language exchange website called Lang-8, and used this corpus to training a statistical machine translator.", "Zampieri and  CITE-p-16-1-11  used a journalistic corpus as the reference corpus and took advantage of the frequent N-grams to detect the errors in the data provided by the shared task.", "NTOU\u2019s submission for the shared task was a traditional supervised one, which extracted word N-grams and POS N-grams as features and trained using SVM ( CITE-p-16-1-5 ).", "Our submission was similar to NTOU\u2019s work whereas we didn\u2019t use any large scale textural corpus as references.", "Our target was to see to what extent can the supervised learner learn only from the limited resource and what types of classifiers perform better in this task.", "We show some of our observations towards the training data in this section.", "What we observed are some frequent cases among the error types Missing and Redundant.", "For the error type Missing, we noticed that errors often occur in some certain cases.", "For example, the auxiliary word \u7684 (of/\u2019s) accounts for 11.35% in all the Missing sentences (and 7.93% sentences contain \u7684 in the training data are incorrect).", "One of the most frequent missing cases is the missing between an adjective (~est for short) and a noun.", "For instance, \u6700\u597d(\u7684) \u96fb\u5f71\u9662 (the best cinema), \u9644\u8fd1(\u7684) \u98ef\u5e97 (a near restaurant), and \u6211(\u7684)\u65e5\u5e38\u751f\u6d3b (my daily life).", "From the English translation we see that there is no \u2018s or of in the phrase such as the girl\u2019s dress ( \u5973\u5b69 \u7684\u8863\u670d ) or a friend of mine ( \u6211\u7684\u4e00\u500b\u670b\u53cb ), but in the grammar of Chinese, a \u7684 is inserted due to the incompleteness of the expressions.", "For the error type Redundant, the word \u4e86 (an auxiliary word related to a perfect tense) accounts for 10.88% in all the Redundant sentences (and 21.78% sentences contain \u4e86 are incorrect).", "The word is redundant when the sentence contains nothing related to a perfect tense.", "For instance, \u6211\u7b2c\u4e00\u6b21\u53bb(\u4e86) \u82f1\u570b\u7559\u5b78 \u3002 (I studied abroad in Britain for the first time.)", "and \u7576\u6642\u4ed6\u4e0d \u8001(\u4e86) \u3002(He wasn\u2019t old at that time.)", ".", "Words that are grammatical incorrect are almost function words, which behave differently in the grammars for Chinese and English (or other languages).", "Typical examples are \u662f (is), \u90fd (auxiliary), \u6709 (be), \u6703 (will), \u5728 (in/at), \u8981 (will), etc.", "However, we didn\u2019t do much towards specific words in our research but only recognize there should be some frequent rules that we can follow.", "In this work, neither did we use any external corpora except for the dataset for the year 2014\u2019s shared task, nor are any language specific heuristic rules or frequent patterns included.", "We were going to see what kind of features and what type of supervised learners can benefit this problem most.", "As declared previously, we did little for the position level extraction, so we introduce mostly on feature extraction, model selection and the construction of the training data.", "For this task, we tried several kinds of features such words, POS (part-of-speech), as well as syntactic parse trees and dependency trees.", "Finally, we find that POS Trigram features perform stably and generate the best results.", "Therefore, we define the POS Trigram for sentential classification at first.", "For each word in a sentence, we extract the following triple as the Trigram for this word: <POS-1, POS, POS+1>.", "And for the beginning and the ending of a sentence, we add two indicators to make up the column vectors.", "For example, in the sentence \u9019/\u4e00\u5929/\u5f88/\u6709\u610f\u601d \u3002(This day is very interesting.)", ", the sentence-level POS features are (r, m, zg, l) the features for the word\u9019 (This) are <start, r,  MATH-w-6-1-2-26  .", "In addition, we extract the relative frequency (probability) for each triple based on the CLP 14 and 15 dataset as  MATH-w-6-1-3-23 .", "In the experiment, we noticed that the frequency features are also good indicators to detect candidates for grammatical errors.", "To summarize, we extract two types of POS Trigram features: the binary Trigram and the probabilistic Trigram.", "The binary Trigram demands that if the sentence contains this Trigram (i.e. <start, r, m>), the corresponding position in the gram vector (the union set of all possible Trigrams after removing those with very low frequencies) is set to be 1.", "For probabilistic Trigram, the position is set to be the relative frequency (the proportion for the Trigram).", "After feature extraction, we put the features into several supervised learners.", "We use a series of single classifiers such as Na\u00efve Bayes (NB), Decision Tree (DT), Support Vector Machines (SVM) and Maximum Entropy (ME), and ensemble learners Adaboost (AB), Random Forest (RF) and Random Feature Subspace (RFS).", "RF is an ensemble of several DTs, each of which samples training instances with replacement and samples features without replacement.", "RFS is an ensemble classifier based on feature sampling which takes results trained on different feature subspace as majority voters.", "We take those training sentences with annotated errors as positive instances and subsample the correct sentences as negative ones.", "Through tuning towards the proportion of negative instances, we discovered that the number of negative instances also affected the final results.", "In the experiment, we use the training data from this year\u2019s and last year\u2019s shared tasks.", "Table 2 lists the number of sentences for each type in the training data.", "Since the scale of this year\u2019s data is really small, we add last year\u2019s corpus into the training data and do cross validations in the training steps.", "Table 2 lists the number of sentences for each error type in these two years\u2019 dataset.", "Our experiments cover training data construction, feature selection and supervised learning.", "We tried several groups of training data, different combinations of features and a variety of classifiers in the training phase.", "As mentioned previously, the sentences that contain no grammatical errors behave as the negative instances for training.", "To avoid imbalance between the positive and negative instances, negative ones were randomly selected to construct the training set.", "At last, we divided the training data into 8 parts and used 8-fold cross validation (CV) for the classifiers.", "We found that, when we selected 4000 negative instances, the system achieved the best results.", "As mentioned in \u00a74.1, we investigate the features POS Trigram and POS Trigram + POS Trigram probability.", "We report the CV results generated by four single classifiers and three ensemble classifiers in Table 3 and Table 4 for the two set of features, respectively.", "The results have been optimized through tuning the parameter settings for each classifier.", "From the results, we find that the ensemble classifiers generally perform better than the single ones, and AB achieves the best results for detection and identification.", "Among the three runs of results we submitted, the first run is the best.", "We show the results in Table 5 and compare them with the CV results.", "This submission is generated by the ensemble classifier RFS by using POS Trigram and probability features.", "We see that the performance of the identification level greatly falls behind that in the cross validation.", "One of the possible reasons for this gap, we consider is the setting of instances, which may be quite distinct between the training and the testing data.", "And another possible reason is the reasonability of the probability features.", "Compare the results generated by the two feature sets (Table 1), it can be seen that the second feature set outperforms the first, on both the detection level and the identification level.", "To some extent, it indicates that the patterns for the grammatical phrases may frequently occur in the datasets.", "Further, we pick up the last four classifiers which perform relatively better on the task data, including DT and three ensemble classifiers, and do statistical analysis on the true positive rates during cross validation (Figure 1).", "The results reveal that the difficulty on judging decreases from Redundant, Missing to Disorder and Selection.", "In addition, the accuracy for the correct label is not quite high, leading to a number of false negative sentences.", "Through observation, we found several cases might affect the predicting results.", "A typical case is that a grammatically wrong sentence can be corrected through several ways, corresponding to more than one error types.", "For example, the sentence \u4ed6 \u99ac \u4e0a \u6e96 \u5099 \u4e0a \u5b78 (He is preparing for school.)", "can be classified to any of the four types: tion.", "All the four directions are reasonable but the dataset only provide the third one.", "Therefore, these data may create confusion for classification and should be considered in the future work.", "In addition, some annotation maybe not so cleat, for instance in the sentence  MATH-w-12-8-1-48  (But these days I noticed some bad habits on you in your daily life).", "The given annotation is selection, but we think redundant is much more reasonable.", "According to the observations towards the training data, we think the following direct proposal is learning from the position level, just as the shared task demands.", "On this level, we can extract more pointed features, integrating both syntactic and semantic ones.", "Besides, for the sentential level classification, the deep neural network based methods (i.e. Convolutional Neural Networks) are expected, with traditional features or embeddings, to detect more structured rules.", "In addition, we deem that dependency tree features may be useful and should be further developed.", "And improvement may also be achieved by mining the confusion in annotation (i.e. the difference between selection and redundant).", "In this paper, we introduce the ensemble learning based method used in the CFL shared task for Chinese grammatical error diagnosis.", "We report some of our observations towards the training data, features and learners we used in our experiments.", "Different from most previous work, we didn\u2019t use any other external language corpus for reference and we didn\u2019t use any rules either.", "The results show that the ensemble methods perform better than the single classifiers based on our simple features."]}], "queries": ["Description of Approach"]}, {"setId": "CoNLL-2014", "papers": [{"summaries": ["Since MT and classifiers differ with respect to the types of errors they can better handle, we combine these systems in a pipeline architecture where the MT is applied to the output of classifiers."], "paperId": "P16-1208", "paperName": "2014-1", "document": ["We focus on two leading state-of-the-art approaches to grammatical error correction \u2013 machine learning classification and machine translation.", "Based on the comparative study of the two learning frameworks and through error analysis of the output of the state-of-the-art systems, we identify key strengths and weaknesses of each of these approaches and demonstrate their complementarity.", "In particular, the machine translation method learns from parallel data without requiring further linguistic input and is better at correcting complex mistakes.", "The classification approach possesses other desirable characteristics, such as the ability to easily generalize beyond what was seen in training, the ability to train without human-annotated data, and the flexibility to adjust knowledge sources for individual error types.", "Based on this analysis, we develop an algorithmic approach that combines the strengths of both methods.", "We present several systems based on resources used in previous work with a relative improvement of over 20% (and 7.4 F score points) over the previous state-of-the-art.", "For the majority of English speakers today, English is not the first language.", "These writers make a variety of grammar and usage mistakes that are not addressed by standard proofing tools.", "Recently, there has been a spike in research on grammatical error correction (GEC), correcting writing mistakes made by learners of English as a Second Language, including four shared tasks: HOO ( CITE-p-17-3-7 ,  CITE-p-17-3-8 ) and", "CoNLL ( CITE-p-17-3-32 ,  CITE-p-17-5-0 ).", "These shared tasks facilitated progress on the problem within the framework of two leading methods \u2013 machine learning classification and statistical machine translation (MT).", "The top CoNLL system combined a rule-based module with MT ( CITE-p-17-3-12 ).", "The second system that scored almost as highly used machine learning classification ( CITE-p-17-5-11 ), and the third system used MT ( CITE-p-17-3-24 ).", "Furthermore,  CITE-p-17-5-12  showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited.", "Despite success of various methods and the growing interest in the task, the key differences between the leading approaches have not been identified or made explicit, which could explain the lack of progress on the task.", "Table 1 shows existing state-of-the-art since CoNLL-2014.", "The top results are close, suggesting that several groups have competitive systems.", "Two improvements (of  MATH-w-2-4-2-683  points) were published since then ( CITE-p-17-5-12 ,  CITE-p-17-3-30 ).", "The purpose of this work is to gain a better understanding of the values offered by each method and to facilitate progress on the task, building on the advantages of each approach.", "Through better understanding of the methods, we exploit the strengths of each technique and, building on existing architecture, develop superior systems within each framework.", "Further combination of these systems yields even more significant improvements over existing state-of-the-art.", "We make the following contributions: bine the strengths of both frameworks and demonstrate substantial progress on the task.", "Specifically, the best system outperforms the previous best result by 7.4 F score points.", "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2205\u20132215, Berlin, Germany, August 7-12, 2016.", "2016c Association for Computational Linguistics", "\u2022 We examine two state-of-the-art approaches to GEC and identify strengths and weaknesses of the respective learning frameworks.", "\u2022 We perform an error analysis of the output of two state-of-the-art systems, and demonstrate how the methods differ with respect to the types of language misuse handled by each.", "\u2022 We exploit the strengths of each framework: with classifiers, we explore the ability to learn from native data, i.e. without supervision, and the flexibility to adjust knowledge sources to specific error types; with MT, we leverage the ability to learn without further linguistic input and to better identify complex mistakes that cannot be easily defined in a classifier framework.", "\u2022 As a result, we build several systems that com-", "Section 2 describes related work.", "Section 3 presents error analysis.", "In Section 4, we develop classifier and MT systems that make use of the strengths of each framework.", "Section 5 shows how to combine the two approaches.", "We first introduce the CoNLL-2014 shared task and briefly describe the state-of-the-art GEC systems in the competition and beyond.", "Next, an overview of the two leading methods is presented.", "CoNLL-2014 training data (henceforth CoNLL-train) is a corpus of learner essays (1.2M words) written by students at the National University of Singapore ( CITE-p-17-3-6 ), corrected and error-tagged.", "The CoNLL-2013 test set was included in CoNLL-2014 and is used as development.", "Both the development and the test sets are also from the student population studying at the same University but annotated separately.", "The annotation includes specifying the relevant correction as well as the information about each error type.", "The tagset consists of 28 categories.", "Table 2 illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and \u2205 denotes a missing word.", "The majority of these errors are related to grammar but also include mechanical, collocation, and other errors.", "An F-based scorer, named M2, was used to score the systems ( CITE-p-17-3-5 ).", "The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall.", "Two types of annotations were used: original and revised.", "We follow the recommendations of the organizers and use the original data ( CITE-p-17-5-0 ).", "The approaches varied widely: classifiers, MT, rules, hybrid systems.", "Table 3 summarizes the top five systems.", "The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical errors.", "As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners\u2019 Corpus and the Lang-8 corpus (Mizumoto et al., 2011), respectively.", "Many teams also used native English datasets.", "The most common ones are the Web1T corpus ( CITE-p-17-3-1 ), the CommonCrawl dataset, which is similar to Web1T, and the English Wikipedia.", "In addition,  CITE-p-17-5-12  made an attempt at combining MT and classifiers.", "They used CoNLL-train and Lang-8 as non-native data and English Wikipedia as native data.", "We believe that the reason this study did not yield significant improvements (Table 1) is that individual strengths of each framework have not been fully exploited.", "Further, each system was applied separately and decisions were combined using a general MT combination technique ( CITE-p-17-3-21 ).", "Finally, Mizumoto and  CITE-p-17-3-30  attempt to improve an MT system also trained on Lang-8 with discriminative reranking using part-of-speech (POS) and dependency features but only obtain a small improvement.", "The statistical machine translation approach is based on the noisy-channel model.", "The best translation for a foreign sentence  MATH-w-5-1-0-20  is:  MATH-p-5-2-0 ", "The model consists of two components: a language model assigning a probability  MATH-w-5-8-0-14  for any target sentence e, and a translation model that assigns a conditional probability  MATH-w-5-8-0-34 .", "The language model is learned using a monolingual corpus in the target language.", "The parameters of the translation model are estimated from a parallel corpus, i.e. the set of foreign sentences and their corresponding translations into the target language.", "In error correction, the task is cast as translating from erroneous learner writing into corrected well-formed English.", "The MT approach relies on the availability of a parallel corpus for learning the translation model.", "State-of-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon ( CITE-p-17-3-26 ).", "The resulting lexicon consists of a list of pairs  MATH-w-5-8-1-37  where  MATH-w-5-8-1-45  is a sequence of one or more foreign words,  MATH-w-5-8-1-57  is a predicted translation.", "Each pair comes with an associated score.", "At decoding time, all phrases from sentence  MATH-w-5-8-1-80  are collected with their corresponding translations observed in training.", "These are scored together with the language modeling scores and may include other features.", "The phrase-based approach by  CITE-p-17-3-26  uses a log-linear model ( CITE-p-17-5-1 ), and the best correction maximizes the following:  MATH-p-5-9-0  where  MATH-w-5-10-0-1  is a feature function, such as language model score and translation scores, and  MATH-w-5-10-0-19  corresponds to a feature weight.", "The classifier approach is based on the context-sensitive spelling correction methodology (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007) and goes back to earlier approaches to article and preposition error correction ( CITE-p-17-3-23 ,  CITE-p-17-3-19 ,  CITE-p-17-3-15 ,  CITE-p-17-3-11 ,  CITE-p-17-5-13 ,  CITE-p-17-3-16 ,  CITE-p-17-3-4 ,  CITE-p-17-3-5 ).", "The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data.", "Classifiers are trained individually for a specific error type.", "Because an error type needs to be defined, typically only well-defined mistakes can be addressed in a straightforward way.", "Given an error type, a confusion set is specified and includes a list of confusable words.", "For some errors, confusion sets are constructed using a closed list (e.g. prepositions).", "For other error types, NLP tools are required.", "To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used.", "Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target.", "In the classifier paradigm, there are various algorithms \u2013 generative ( CITE-p-17-3-16 ,  CITE-p-17-5-2 ), discriminative ( CITE-p-17-3-19 ,  CITE-p-17-3-15 ,  CITE-p-17-3-11 ,  CITE-p-17-5-13 ), and joint approaches ( CITE-p-17-3-5 ,  CITE-p-17-5-8 ).", "Earlier works trained on native data (due to lack of annotation).", "Later approaches incorporated learner data in training in various ways ( CITE-p-17-3-20 ,  CITE-p-17-3-16 ,  CITE-p-17-5-5 ,  CITE-p-17-3-4 ).", "This section presents error analysis of the MT and classifier approaches.", "We begin by identifying several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems:", "(1a) Error coverage denotes the ability of a system to identify and correct a variety of error types.", "(1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact.", "(2) Generalizibility refers to the ability of a system to identify mistakes in new unseen contexts and propose corrections beyond those observed in training data.", "(3) The role of supervision or having annotated learner data for training.", "(4) System flexibility is a property of the system that allows it to adapt resources specially to correct various phenomena.", "The two paradigms are summarized in Table 4.", "We use + and \u2212 to indicate whether a learning framework has desirable (+) or undesirable characteristic with regard to each factor.", "The first three properties characterize system output, while (3) and (4) arise from the system frameworks.", "Below we analyze the output of several state-of-the-art CoNLL-2014 systems in more detail.", "1 Section 4 explores (3) and (4) that relate to the learning frameworks.", "Error coverage To understand how systems differ with respect to error coverage, we consider recall of each system per error type.", "Error-type recall can be easily computed using error tags and is reported in the CoNLL overview paper.", "The recall numbers show substantial variations among the systems.", "If we consider error categories that have non-negligible recall numbers (higher than 10%), classifier-based approaches have a much lower proportion of error types for which 10% recall was achieved.", "Among the 28 error types, the top classifier systems \u2013 Columbia University-University of Illinois (CUUI, top-2) and National Tsing Hua University (NTHU, top- 5) \u2013 have a recall higher than 10% for 8 and 9 error types, respectively.", "In contrast, the two MT-based systems \u2013 Cambridge University (CAMB, top-1) and the Adam Mickiewicz University system (AMU, top-3) \u2013 have 15 and 17 error types, respectively, for which the recall is at least 10%.", "These recall discrepancies indicate that the MT approach has a better overall coverage, which is intuitive given that all types of confusions are automatically added through phrase-based translation tables in MT, while classifiers must explicitly model each error type.", "Note, however, that these numbers do not necessarily indicate good type-based performance, since high recall may correspond to low precision.", "Error complexity In the MT approach, error confusions are learned automatically via the phrase translation tables extracted from the parallel training data.", "Thus, an MT system can easily handle interacting and complex errors where replacements involve a sequence of words.", "Table 5 illustrates complex and interacting mistakes that the MT approach is able to handle.", "Example (1) contains a phrase-level correction that includes both a preposition replacement and an adjective change.", "(2) is an instance of an interacting mistake where there is a dependency between the article and the noun number, and a mistake can be corrected by changing one of the properties but not both.", "(3), (4) and (5) require multiple simultaneous corrections on various words in a phrase.", "Because MT systems extract error/correction pairs from phrase-translation tables, they can only identify erroneous surface forms observed in training and propose corrections that occurred with the corresponding surface forms.", "Crucially, in a standard MT scenario, any resulting translation consists of \u201cmatches\u201d mined from the translation tables, so a standard MT model lacks lexical abstractions that might help generalize, thus out-of-vocabulary words is a well-known problem in MT ( CITE-p-17-3-9 ).", "While more advanced MT models can abstract by adding higher-level features such as POS, previous attempt yielded only marginal improvements (Mizumoto and Matsumoto, 2016), since one typically needs different types of abstractions depending on the error type, as we show below.", "With classifiers, it is easy to generalize using higher-level information that goes beyond surface form and to adjust the abstraction to the error type.", "Many grammatical errors may benefit from generalizations based on POS or parse information; we can thus expect that classifiers will do better on errors that require linguistic abstractions.", "To validate this hypothesis, we evaluate type-based performance of two systems: a top-3 MT-based AMU system and a top-2 classifier-based CUUI; we do not include the top-1 system, since it is a hybrid system that also uses rules.", "Unlike recall, estimating type-based precision requires knowing the type of the correction supplied by the system, which is not specified in the output.", "We thus manually analyze the output of the AMU and CUUI systems for seven common error categories and assign to each correction an appropriate type to estimate precision and F0.5 (Table 6).", "The CUUI system addresses all of these errors, with the exception of mechanical (Mec), of which it handles a small subset.", "The AMU system does better on mechanical, preposition, word form, and noun number.", "We now consider examples of errors that are corrected by the classifier-based CUUI system in these three categories but are missed by the MT-based AMU system (Table 7).", "Examples (1) and (2) illustrate verb errors with long-distance subjects (\u201cone\u201d and \u201cpatients\u201d).", "This is handled in the classification approach via syntactic features.", "An MT system misses these errors because it is limited to edits within short spans.", "Examples (3), (4), and (5) illustrate verb mistakes for which the correct replacements were not observed in training but that are nonetheless corrected by generalizing beyond surface form.", "Finally, (6) and (7) illustrate omission and insertion errors, a majority of article mistakes.", "The MT system is especially bad at correcting such mistakes.", "Notably, the classifier-based CUUI system correctly identified twice as many omitted articles and more than 20 times more superfluous articles than the MT-based AMU system.", "This happens because an MT system is restricted to suggesting deletions and insertions in those contexts that were observed in training, whereas a classifier uses shallow parse information, which allows it to insert or delete an article in front of every eligible noun phrase.", "These examples demonstrate that the ability of a system to generalize beyond the surface forms is indeed beneficial for long-distance dependencies, for abstracting away from surface forms when formulating confusion sets, and for mistakes involving omitting or inserting a word.", "In this section, we explore the advantages of each learning approach, as identified in the previous section, within each learning framework.", "To this end, drawing on the strengths of each framework, we develop new state-of-the-art MT and classifier systems.", "MT and classifier components and show how to exploit the strengths of each framework in combination.", "Table 8 summarizes the data used.", "Results are reported with respect to all errors in the test data.", "This is different from performance for individual errors in Table 6.", "A key advantage of the MT framework is that, unlike with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input.", "We build two MT systems that differ only in the use of parallel data: the CoNLL- 2014 training data and Lang-8.", "Our MT systems are trained using Moses ( CITE-p-17-3-27 ) and follow the standard approach ( CITE-p-17-3-24 ,  CITE-p-17-5-12 ).", "Both systems use two 5-gram language models \u2013 English Wikipedia and the corrected side of CoNLL-train \u2013 trained with KenLM ( CITE-p-17-3-22 ).", "Table 9 reports the performance of the systems.", "As shown, performance increases by more than 11 points when a larger parallel corpus is used.", "We now present several classifier systems, exploring the two important properties of the classification framework \u2013 the ability to train without supervision and system flexibility (see Table 4).", "Supervision in the form of annotated learner data plays an important role in developing an error correction system but is expensive.", "Native data, in contrast, is cheap and available in large quantities.", "Therefore, the fact that, unlike with MT, it is possible to build a classifier system without any annotated data, is a clear advantage of classifiers.", "Training without supervision is possible in the classification framework, as follows.", "For a given mistake type, e.g. preposition, a classifier is trained on native data that is assumed to be correct; the classifier uses context words around each preposition as features.", "The resulting model is then applied to learner prepositions and will predict the most likely preposition in a given context.", "If the preposition predicted by the classifier is different from what the author used in text, this preposition is flagged as a mistake.", "We refer the reader to Rozovskaya and  CITE-p-17-5-6  and Rozovskaya and  CITE-p-17-5-7  for a description of training classifiers with and without supervision for error correction tasks.", "\u2022 Training with supervision: When training using learner data, how does a classifier-based system compare against an MT system?", "\u2022 Training without supervision: How well can we do by building a classifier system with native data only, compared to MT and classifier-based systems that use supervision?", "Our classifier system is based on the implementation framework of the second CoNLL-2014 system ( CITE-p-17-5-11 ) and consists of classifiers for 7 most common grammatical errors in CoNLL-train: article; preposition; noun number; verb agreement; verb form; verb tense; word form.", "All modules take as input the corpus documents preprocessed with a POS tagger 3 (Even-Zohar and Roth, 2001), a shallow parser 4 (Punyakanok and Roth, 2001), a syntactic parser ( CITE-p-17-3-25 ) and a dependency converter ( CITE-p-17-3-29 ).", "Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T).", "Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm ( CITE-p-17-5-4 ) and use rich POS and syntactic features tailored to specific error types that are standard for these tasks (Lee and Seneff, 2008; Han et al., 2006; Tetreault et al., 2010; Rozovskaya et al., 2011); Na\u0131\u0308ve Bayes classifiers are trained on Web1T with word n-gram features.", "A detailed description of the classifiers and the features used can be found in Rozovskaya and  CITE-p-17-5-9 .", "Table 10 shows the performance of two classifier systems, trained with supervision (on CoNLL-train) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train.", "The first classifier system performs comparably to the MT system (27.76 vs. 28.25), however, the native-trained classifier system outperforms both, and does not use any annotated data.", "The native-trained classifier system would place fourth in CoNLL-2014.", "We now explore another advantage of the classifier-based approach, that of allowing for a flexible architecture where we can tailor knowledge sources for individual phenomena.", "In Section 4.2.1, we already took advantage of the fact that in the classifier framework it is easy to incorporate features suited to individual error types.", "We now show that by adding supervision in a way tailored toward specific errors we can further improve the classifier-based approach.", "Adding Supervision in a Tailored Way There is a trade-off between training on native and learner data.", "The advantage of training on native data is clearly the size, which is important for estimating context parameters.", "Learner data provides additional information, such as learner error patterns and the manner of non-native writing.", "Instead of choosing to train on one data type, the classifier framework allows one to combine the two data sources in various ways: voting (Rozovskaya et al., 2014), alternating structure optimization ( CITE-p-17-3-4 ), training a meta-classifier ( CITE-p-17-3-16 ), and extracting error patterns ( CITE-p-17-5-7 ).", "We compare two approaches of adding supervision: (1) Learner error patterns: Error patterns are extracted from learner data and \u201cinjected\u201d into models trained on native data ( CITE-p-17-5-7 ).", "Learner data is used to estimate mistake parameters; contextual cues are based on native data.", "(2) Learner error patterns+native predictions: Classifiers are trained on native data.", "Classifier predictions are used as features in models trained on learner data.", "Learner data thus contributes both the specific manner of learner writing and the mistake parameters.", "We found that (2) is superior to (1) for article, agreement, and preposition errors; (1) works better on verb form and word form errors; and noun number errors perform best when a classifier is trained on native data.", "(Learner error patterns were found not to be beneficial for correcting noun number errors ( CITE-p-17-5-9 )).", "Tailored supervision yields an improvement of almost 3 points over the system trained on native data and almost 9 points over the system trained on learner data (Table 11).", "Adding Mechanical Errors Finally, we add components for mechanical errors: punctuation, spelling, and capitalization.", "These are distinguished from the grammatical mistakes, as they are not specific to GEC and can be handled with existing resources or simple methods.", "For capitalization and missing commas, we compile a list of patterns using CoNLL training data.", "We also use an off-the-shelf speller ( CITE-p-17-3-14 ,  CITE-p-17-3-13 ).", "Results are shown in Table 12.", "Performance improves by almost 5 and 7 points for the native-trained system and for the best configuration of classifiers with supervision.", "Both systems also outperform the top CoNLL system, by 1 and 6 points, respectively.", "The result of 43.11 by the best classifier configuration substantially outperforms the existing state-of-the-art: a combination of two MT systems and two classifier systems, and MT with reranking ( CITE-p-17-5-12 ,  CITE-p-17-3-30 ).", "Since MT and classifiers differ with respect to the types of errors they can better handle, we combine these systems in a pipeline architecture where the MT is applied to the output of classifiers.", "Classifiers are applied first, since MT is better at handling complex phenomena.", "First, we add the speller and those classifier components that perform substantially better than MT (articles and verb agreement), due to the ability of classifiers to generalize beyond lexical information.", "Results are shown in Table 13.", "Adding classifiers improves the performance, thereby demonstrating that the classifiers address a complementary set of mistakes.", "Adding all three modules improves the results from 28.25 to 40.10 and from 39.48 to 46.51 for the MT systems trained on CoNLL-train and Lang-8, respectively.", "Notably, the CoNLL-train MT system especially benefits, which shows that when the parallel data is small, it is particularly worthwhile to add classifiers.", "It should be stressed that even with a smaller parallel corpus, when the three modules are added, the resulting system is very competitive with previous state-of-the-art that uses a lot more supervision:  CITE-p-17-5-12  and Mizumoto and  CITE-p-17-3-30  use Lang-8.", "These results show that when one has an MT system, it is possible to improve by investing effort into building select classifiers for phenomena that are most challenging for MT.", "Finally, Table 14 demonstrates that combining MT with the best classifier system improves the result further when the MT system is trained on Lang-8, but not when the MT system is trained on CoNLL-train.", "We also note that the CoNLL-train MT system also has a much lower precision than the other systems.", "We conclude that when only a limited amount of data is available, the classifier approach on its own performs better.", "As a summary, Table 15 lists the best systems developed in this work \u2013 a classifier system, a pipeline of select classifiers and MT, and a pipeline consisting of the best classifier and the MT systems \u2013 and compares to existing state-of- the-art.", "Our classifier system is a 3-point improvement over the existing state-of-the-art, while the best pipeline is a 7.4-point improvement (20% relative improvement).", "A recent surge in GEC research has produced two leading state-of-the-art approaches \u2013 machine learning classification and machine translation.", "Based on the analysis of the methods and an error analysis on the outputs of state-of-the-art systems that adopt these approaches, we explained the differences and the key advantages of each.", "With respect to error phenomena, we showed that while MT is better at handling complex mistakes, classifiers are better at correcting mistakes that require abstracting beyond lexical context.", "We further showed that the key strengths of the classification framework are its flexibility and the ability to train without supervision.", "We built several systems that draw on the strengths of each approach individually and in a pipeline.", "The best classifier system and the pipelines outperform reported best results on the task, often by a large margin.", "The purpose of this work is to gain a better understanding of the advantages offered by each learning method in order to make further progress on the GEC task.", "We showed that the values provided by each method can be exploited within each approach and in combination, depending on the resources available, such as annotated learner data (MT), and additional linguistic resources (classifiers).", "As a result, we built robust systems and showed substantial improvement over existing state-of-the-art.", "For future work, we intend to study the problem in the context of other languages.", "However, it is important to realize that the problem is far from being solved even in English, and the current work makes very significant progress on it."]}, {"summaries": ["We have shown that NMT can be successfully applied to GEC once we address the rare word problem."], "paperId": "N16-1042", "paperName": "2014-2", "document": ["This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC).", "We propose a two-step approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task.", "Our best NMT-based system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set.", "The same system achieves an F 0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively.", "Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in text written by non-native English writers.", "Unlike building machine learning classifiers for specific error types (e.g. determiner or preposition errors) ( CITE-p-14-3-20 ,  CITE-p-14-3-16 ,  CITE-p-14-1-6 ), the idea of \u2018translating\u2019 a grammatically incorrect sentence into a correct one has been proposed to handle all error types simultaneously ( CITE-p-14-3-1 ,  CITE-p-14-3-3 ).", "Statistical machine translation (SMT) has been successfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task ( CITE-p-14-3-12 ).", "Recently, several neural machine translation (NMT) models have been developed with promising results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "Unlike SMT, which consists of components that are trained separately and combined during decoding (i.e. the translation model and language model) ( CITE-p-14-3-7 ), NMT learns a single large neural network which inputs a sentence and outputs a translation.", "NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the training set more effectively ( CITE-p-14-3-8 ).", "NMT-based systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC.", "However, NMT models typically limit vocabulary size on both source and target sides due to the complexity of training (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Jean et al., 2015).", "Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are replaced with UNK symbol.", "This problem is more serious for GEC as non-native text contains not only rare words (e.g. proper nouns), but also misspelled words (i.e. spelling errors).", "By replacing all the OOV words with the same UNK symbol, useful information is discarded, resulting in systems that are not able to correct misspelled words or even keep some of the error-free original words, as in the following examples (OOV words are underlined):", "Original sentence ... I am goign to make a plan ...", "System hypothesis ... I am UNK to make a plan ...", "Gold standard", "Proceedings of NAACL-HLT 2016, pages 380\u2013386, San Diego, California, June 12-17, 2016.", "2016c Association for Computational Linguistics", "... I am going to make a plan ...", "Original sentence", "I suggest you visit first the cathedral of \u201c Le Seu d\u2019Mrgell \u201d because it is the most emblematic building in the area .", "System hypothesis", "I suggest you visit first the cathedral of \u201c Le UNK UNK \u201d because it is the most UNK building in the area .", "Gold standard I suggest you visit first the cathedral of \u201c Le Seu d\u2019Mrgell \u201d because it is the most emblematic building in the area .", "Inspired by the work of  CITE-p-14-3-8 , we propose a similar but much simpler two-step approach to address the rare word problem: rather than annotating the training data with alignment information, we apply unsupervised alignment models to find the sources of the words in the target sentence.", "Once we know the source words that are responsible for the unknown target words, a word level translation model learnt from parallel sentences is used to translate these source words.", "This paper makes the following contributions.", "First, we present the first study using NMT for GEC, outperforming the state-of-the-art.", "Second, we propose a two-step approach to address the rare word problem in NMT for GEC, which we show yields a substantial improvement.", "Finally, we report results on two well-known publicly available test sets that can be used for cross-system comparisons.", "NMT systems apply the so-called encoder-decoder mechanism proposed by  CITE-p-14-1-5  and  CITE-p-14-3-19 .", "An encoder reads and encodes an entire source sentence  MATH-w-3-1-0-35  into a vector  MATH-w-3-1-0-54 :  MATH-p-3-2-0 ", "A decoder then outputs a translation  MATH-w-3-4-0-6  by predicting the next word  MATH-w-3-4-0-28  based on the encoded vector  MATH-w-3-4-0-35  and all the previously predicted words  MATH-w-3-4-0-43 :  MATH-p-3-5-0  where  MATH-w-3-6-0-1  is the hidden state of the decoder.", "Different neural network models have been proposed, for example, Kalchbrenner and Blunsom (2013) proposed a hybrid of a recurrent neural network (RNN) and a convolutional neural network,  CITE-p-14-3-19  used a Long Short-Term Memory (LSTM) model,  CITE-p-14-1-5  proposed a similar but simpler gated RNN model, and Bahdanau et al. (2014) introduced an attentional-based architecture.", "In this work, we use the RNNsearch model of Bahdanau et al. ( CITE-p-14-1-0 ), which contains a bidirectional RNN as an encoder and an attention-based decoder.", "The bidirectional RNN encoder has a forward and a backward RNN.", "The forward RNN reads the source sentence from the first word to the last, and the backward RNN reads the source sentence in reverse order.", "By doing this, it captures both historical and future information.", "The attention-based model allows the decoder to focus on the most relevant information in the source sentence, rather than remembering the entire source sentence.", "The rare word problem in NMT has been noticed by (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015;  CITE-p-14-3-2 .", "Jean et al. (2015) proposed a method based on importance sampling that uses a very large target vocabulary without increasing training complexity.", "However, no matter how large the target vocabulary size is, there are still OOV words.", "We also notice that in GEC, the source side vocabulary size is much larger than that of the target side as there are many incorrect words in the source (e.g. spelling mistakes and word form errors) (see Section 4.1).", " CITE-p-14-3-8  introduced three new annotation strategies to annotate the training data, so that unknown words in the output can be traced back to their origins.", "The training data was first re-annotated using the output of a word alignment algorithm.", "NMT systems were then built using this new data.", "Finally, information about the OOV words in the target sentence and their corresponding words in the source sentence was extracted from the NMT systems and used in a post-processing step to translate these OOV words using a dictionary.", "We propose a similar two-step approach: 1) aligning the unknown words (i.e. UNK tokens) in the target sentence to their origins in the source sentence with an unsupervised aligner; 2) building a word level translation model to translate those words in a post-processing step.", "In order to locate the source words that are responsible for the unknown target words, we apply unsupervised aligners directly and use only the NMT model output instead of first re-annotating training data, and then building new NMT models using this newly annotated data as proposed by  CITE-p-14-3-8 .", "Our approach is much simpler as we avoid re-annotating any data and train only one NMT model.", "Due to the nature of error correction (i.e. both source and target sentences are in the same language), most words translate as themselves, and errors are often similar to their correct forms.", "Thus, unsupervised aligners can be successfully used to align the unknown target words.", "Two automatic alignment tools are used: GIZA++ ( CITE-p-14-3-14 ) and METEOR ( CITE-p-14-1-1 ).", "GIZA++ is an implementation of IBM Models 1-5 ( CITE-p-14-1-4 ) and a Hidden-Markov alignment model (HMM) ( CITE-p-14-5-0 ), which can align two sentences from any languages.", "Unlike GIZA++, METEOR aligns two sentences from the same language.", "The latest METEOR 1.5 only supports a few languages, and English is one of them.", "METEOR identifies not only words with exact matches, but also words with identical stems, synonyms, and unigram paraphrases.", "This is useful for GEC as it can deal with word form, noun number, and verb form corrections that share identical stems, as well as word choice corrections with synonyms or unigram paraphrases.", "We use the publicly available FCE dataset (Yannakoudakis et al., 2011), which is a part of the Cambridge Learner Corpus (CLC) ( CITE-p-14-3-13 ).", "The FCE dataset contains 1,244 scripts produced by learners taking the First Certificate in English (FCE) examination between 2000 and 2001.", "The texts have been manually annotated by linguists using a taxonomy of approximately 80 error types.", "The publicly available FCE dataset contains about 30,995 pairs of parallel sentences for training (approx.", "496,567 tokens on the target side) and about 2,691 pairs of parallel sentences for testing (approx.", "41,986 tokens on the target side).", "Since the FCE training set is too small to build good MT systems, we add training examples extracted from the CLC.", "Overall, there are 1,965,727 pairs of parallel sentences in our training set.", "The source side contains 28,823,615 words with 248,028 unique words, and the target side contains 29,219,128 words with 143,852 unique words.", "System performance is evaluated using three automatic evaluation metrics: I-measure ( CITE-p-14-1-10 ),  MATH-w-7-1-0-20  Scorer ( CITE-p-14-1-7 ) and GLEU ( CITE-p-14-3-9 ).", "In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected (i.e. the source).", "The  MATH-w-7-1-0-78  Scorer was the official scorer in the CoNLL shared tasks ( CITE-p-14-5-2 ,  CITE-p-14-3-12 ), with F 0.5 being the reported metric in the 2014 edition.", "GLEU is a simple variant of BLEU ( CITE-p-14-3-15 ), which shows better correlation with human judgments on the CoNLL- 2014 shared task test set.", "Following previous work (e.g.  CITE-p-14-1-3 , Yuan and  CITE-p-14-5-2 ), we build a phrase-based SMT error correction system as the baseline.", "Pialign ( CITE-p-14-3-10 ) is used to create a phrase translation table.", "In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Felice et al. (2014).", "Decoding is performed using Moses ( CITE-p-14-3-6 ).", "The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences.", "The IRSTLM Toolkit ( CITE-p-14-1-9 ) is used to buid a 5-gram language model with modified Kneser-Ney smoothing ( CITE-p-14-3-5 ).", "Our training procedure and hyperparameters for the NMT system are similar to those used by  CITE-p-14-1-0 .", "We train models with sentences of length up to 100 words, which covers about 99.96% of all the training examples.", "In terms of vocabulary size, we limit the target vocabulary size to 30K, and experiment with three different source vocabulary sizes: 30K, 50K and 80K.", "The output sentences from the NMT systems are aligned with their source sentences using GIZA++.", "In addition, alignment information learnt by METEOR is used by GIZA++ during aligning.", "All the UNK tokens in the output sentences are replaced with the translation of the source words that are responsible for those UNK tokens.", "The translation is performed using a word level model learnt from IBM Model 4.", "From the results in Table 1, we can see that NMT-based systems alone are not able to achieve comparable results to an SMT-based system.", "It is probably because of the rare word problem, as increasing the source side vocabulary size helps.", "The performance of the best NMT system alone (NMT 80K- 30K), without replacing UNK tokens, is still worse than the SMT baseline.", "When we replace the UNK tokens in the NMT output, using GIZA++ for unknown word alignment improves the system performance for all three NMT systems in all three evaluation metrics.", "We can see that our proposed approach is more useful for NMT systems trained on a small source side vocabulary (e.g. 30K) than a large vocabulary (e.g. 50K, 80K).", "The larger the vocabulary size, the smaller the gain after replacing UNK tokens.", "The introduction of the METEOR alignment information to GIZA++ yields further improvements.", "Our best system (NMT 30K-30K + GIZA++ + METEOR) achieves an F 0.5 score of 53.49%, an I score of 3.94%, and a GLEU score of 71.16%, outperforming the SMT baseline in all three evaluation metrics.", "Comparing the output of the SMT baseline with that of the NMT system reveals that there are some learner errors which are missed by the SMT system but are captured by the NMT system.", "One possible reason is that the phrase-based SMT system is trained on surface forms and therefore unaware of syntactic structure.", "In order to make a correction, it has to have seen the exact correction rule in the training data.", "Since the NMT system does not rely on any correction rules, in theory, it should be able to make any changes as long as it has seen the words in the training data.", "Original sentence", "There are kidnaps everywhere and not all of the family can afford the ransom ...", "SMT hypothesis", "There are kidnaps everywhere and not all of the families can afford the ransom ...", "NMT hypothesis There are kidnappings everywhere and not all of the families can afford the ransom ...", "Gold standard There are kidnappings everywhere and not all of the families can afford the ransom ...", "The SMT system fails to correct the word form error as the correction rule (kidnaps \u2192 kidnappings) is not in the SMT phrase table learnt from the training data.", "Since these two words (kidnaps and kidnappings) have been seen in the training data, the NMT system corrects this error successfully.", "The CoNLL-2014 shared task on grammatical error correction required participating systems to correct all errors present in learner English text.", "The official training and test data comes from the National University of Singapore Corpus of Learner English (NUCLE) ( CITE-p-14-1-8 ).", "F 0.5 was adopted as the evaluation metric, as reported by the  MATH-w-11-1-0-68  Scorer.", "In order to test how well our system generalises, we apply our best system trained on the CLC to the CoNLL-2014 shared task test data directly without adding the NUCLE training data or tuning for the NUCLE.", "The state-of-the-art F 0.5 score was reported by  CITE-p-14-3-18  after the shared task.", "By combining the outputs from two classification-based systems and two SMT-based systems, they achieved an F 0.5 score of 39.39%.", "Results of the uncorrected baseline, our best NMT-based system,  CITE-p-14-3-18 \u2019s system and the top three systems in the shared task are presented in Table 2.", "We can see that our NMT-based system outperforms the top three teams, achieving the highest F 0.5 , I and GLEU scores.", "It also outperforms the state-of-the-art combined system from  CITE-p-14-3-18 .", "Our system achieves the best F 0.5 score of 39.90% even though it is not trained on the NUCLE data.", "We have shown that NMT can be successfully applied to GEC once we address the rare word problem.", "Our proposed two-step approach for UNK replacement has been proved to be effective, and to provide a substantial improvement.", "We have developed an NMT-based system that generalises well to another dataset.", "Our NMT system achieves an F 0.5 score of 53.49%, an I score of 3.94%, and a GLEU score of 71.16% on the publicly available FCE test set, outperforming an SMT-based system in all three metrics.", "When testing on the official CoNLL-2014 test set without alternative answers, our system achieves an F 0.5 score of 39.90%, outperforming the current state-of-the-art.", "In future work, we would like to explore other ways to address the rare word problem in NMT-based GEC, such as incorporating the soft-alignment information generated by the attention-based decoder, or using character-based models instead of word-based ones."]}], "queries": ["Description of Approach"]}]